# Module 03: BigQuery Data Engineering Agent for Data Warehousing 

## Motivation recap:
Shoonya, our hypothetical retail chain understands that to stay competitive, adopt cutting edge/innovative technologies and accelerate speed to production, they need to use the latest and greatest LLM powered product features. Shoonya's architecture board has approved the Data Engineering Agent for use. The IT leadership would like data engineers to get well versed with the BigQuery Data Engineering Agent (DEA) - and have targeted for their data engineers to explore the DEA to build their warehouse dimensional model.

This module provides exactly such an immersive learning experience and via the Dataform UI (DEA API is in development).

<hr>

## Module scope:

This module features the Data Engineering Agent for `Data Warehouse dimensional model code generation with just prompts`, and against the sample retail data in the dataset `rscw_oltp_stg_ds`. It also covers executing the generated code in Dataform.

<hr>

## About Data Engineering Agent

### Product overview

The Data Engineering Agent lets you use Gemini in BigQuery to build, modify, and manage data pipelines to load and process data in BigQuery. With the Data Engineering Agent, you can use natural language prompts to generate data pipelines from various data sources or adapt existing data pipelines to suit your data engineering needs. <br>

The Data Engineering Agent has the following features:
1. Natural language for pipeline creation: The agent uses Gemini to understand your data and interpret your plain-language instructions. You can use plain-language instructions to have the Data Engineering Agent build or edit data pipelines.

2. Dataform integration: The agent generates and organizes the necessary pipeline code into SQLX files within a Dataform repository. The agent operates in the Dataform workspace, so Dataform pipelines are automatically available to the agent.

3. Custom agent instructions: Create agent instructions in plain-language to define custom rules for the Data Engineering Agent. Agent instructions are persistent across your organization, and can be useful to enforce organization-wide rules, such as naming conventions or style guides.

4. Pipeline validation: The agent validates any generated code to ensure that the data pipelines are functional.
<br>

You can use natural language prompts with the Data Engineering Agent to create tables, views, assertions, declarations and operations SQLX files. For example, you can use the Data Engineering Agent to do the following:

- Load data from external data sources such as Cloud Storage in various formats, like CSV, AVRO, or PARQUET.
- Create or use existing BigQuery routines (UDFs) to perform custom analysis and transformations on your data.
- Define reusable guidelines for the agent in natural language.


### Data Engineering Agent API for programmatic access

The REST endpoint for programmatic execution was being overhauled at the time of the authroing this lab. We will therefore do some pre-work in a notebook and then use the Dataform UI for invoking the Data Engineering Agent to generate the dimensional model.

<hr>

### Best practice

1. Follow the sequence for agentic grounding and to avoid hallucinations - run profiling, then Data Insights table-level, followed by dataset-level and then work with the Data Engineering Agent
2. Be cognizant of the fact that the DEA's code generation is not deterministic and apply due diligence for production workloads. Use it to get a baseline, but firm it up with Data Warehousing engineering specialists for production.
3. Use the DEA for baseline code, but not for `generate and directly deploy to production`.

<hr>

## Public documentation

[https://docs.cloud.google.com/bigquery/docs/data-engineering-agent-pipelines](https://docs.cloud.google.com/bigquery/docs/data-engineering-agent-pipelines)


<hr>

## Duration:

This module should take no more than 10 minutes.

<hr>

## Prerequisites:

Completion of Module 02 a-b-c.

<hr>

## Table of contents

| # | Learning unit | 
| -- | :--- | 
| 1 | [Incremental permissions / configurations](Module-03-Data-Engineering-Agent-For-Warehousing.md#unit-1-incremental-permissions--configurations) |
| 2 | [Developer experience](Module-03-Data-Engineering-Agent-For-Warehousing.md#unit-2-developer-experience) |
| 3 | [Author the instructions for the Data Engineering Agent](Module-03-Data-Engineering-Agent-For-Warehousing.md#unit-3-create-instruction-files-for-the-data-engineering-agent) |
| 4 | [Run the code generated by the Data Engineering Agent](Module-03-Data-Engineering-Agent-For-Warehousing.md#unit-4-review-the-instructions-files-generated-in-the-dataform-workspace-ui) |

<hr>


# Lab module

## Unit 1: Incremental permissions, & configurations

### 1.1. Incremental permissions needed

To get the permissions that you need to use the Data Engineering Agent, grant yourself the following:
- Dataform Code Editor (roles/dataform.codeEditor)
- BigQuery Job User (roles/bigquery.jobuser)


Let us grant the user managed service account and ourselves the roles by pasting the below in Cloud Shell:<br>

```
PROJECT_ID=`gcloud config list --format "value(core.project)" 2>/dev/null`
PROJECT_NBR=`gcloud projects describe $PROJECT_ID | grep projectNumber | cut -d':' -f2 |  tr -d "'" | xargs`
UPN_FQN=`gcloud auth list --filter=status:ACTIVE --format="value(account)"`
UMSA="rscw-umsa"
UMSA_FQN="$UMSA@$PROJECT_ID.iam.gserviceaccount.com"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:$UMSA_FQN" \
  --role="roles/dataform.codeEditor"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="user:$UPN_FQN" \
  --role="roles/dataform.codeEditor"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="user:$UPN_FQN" \
  --role="roles/bigquery.jobuser"
```

<hr>

### 1.2. APIs 

API enabling needed:<br>
1. Data Analytics API with Gemini

Paste the below in Cloud Shell:<br>
```
gcloud services enable geminidataanalytics.googleapis.com
```

<hr>


## Unit 2. Developer Experience

The developer continuum is as follows:<br>
1. Create instructions for the Data Engineering Agent to generate a dimensional model / star schema.
2. Have the agent generate code
3. Review the code
4. Run the code
5. Review the data warehouse tables created
6. Tweak, refine instructions and rerun #1 until you see the desired warehousing code generated
7. Operationalize the code to run on schedule


<hr>

## Unit 3. Create instruction files for the Data Engineering Agent

For agentic grounding, we learned in Module 02-a-b-c, that as a best practice, we should run Data Insights. <br>
We will create instruction files off of the Data Insights and also generate a naming conventions file and persist these to the Dataform workspace.<br>

### 3.1. Lets upload a notebook for this learning unit.

![README](../04-images/M03-00-1.png)  

<br><br>

![README](../04-images/M03-00-2.png)  

<br><br>

<hr>

### 3.2. Run the notebook to completion

Once you are done running the notebook, return here for further instructions.

<hr>


## Unit 4: Review the instructions files generated in the Dataform workspace UI

### 4.1. Navigate to the Dataform UI, and into the workspace

![README](../04-images/M03-01.png)  

<br><br>

![README](../04-images/M03-02.png)  

<br><br>

![README](../04-images/M03-03.png)  

<br><br>

![README](../04-images/M03-04.png)  

<br><br>

<hr>

### 4.2. Review the instructions files generated from the notebook execution

![README](../04-images/M03-05a.png)  

<br><br>

![README](../04-images/M03-05b.png)  

<br><br>

![README](../04-images/M03-05c.png)  

<br><br>

![README](../04-images/M03-05d.png)  

<br><br>

<hr>

## Unit 5: Instruct the Data Engineering Agent to generate the star schema & review the code generated

### 5.1. Prompt for the Data Engineering Agent

The following is the prompt you will use in 5.2.
```
You are an expert warehouse data modeler and data engineer. The business context is retail sales and inventory optimization. You are responsible for generating a star schema based off of the instructions and grounding information provided to you with pipeline code that can be run in Dataform"
Instructions provided to you are: 
(1) Naming conventions are available in: 01-naming-conventions.md". Strictly follow these conventions.
(2) Metadata for grounding such as dataset description, table descriptions, column descritions and table relationships are provided to you in: 02-agent-grounding.md". 
Discrete tasks to be completed by you are:"
1. Create a star schema data warehouse structure based on the tables from the rscw_oltp_stg_ds dataset
2. Create the star schema into a new dataset called rscw_dwh_ds. Suggest Dataform pipelines to extract the data from the rscw_oltp_stg_ds dataset tables.
3. Make sure that the string 'data-warehouse' and the source table names appear as tags in the config block of the .sqlx definition files.

```

### 5.2. Launch the Agent UI in the Dataform workspace & paste the prompt

Follow the screenshots below and paste the prompt from 5.1 into the Agent UI -

![README](../04-images/M03-06.png)  

<br><br>

![README](../04-images/M03-07.png)  

<br><br>


<hr>

### 5.3. Observe the Agent's frequent posts on what it is thinking/deducing/executing to completion

Here are the author's screen captures.

![README](../04-images/M03-08.png)  

<br><br>

![README](../04-images/M03-09.png)  

<br><br>

![README](../04-images/M03-10.png)  

<br><br>

![README](../04-images/M03-11.png)  

<br><br>

![README](../04-images/M03-12.png)  

<br><br>

![README](../04-images/M03-13.png)  

<br><br>

![README](../04-images/M03-14.png)  

<br><br>

![README](../04-images/M03-15.png)  

<br><br>

![README](../04-images/M03-15b.png)  

<br><br>

<hr>

### 5.4. Review the code generated


![README](../04-images/M03-16.png)  

<br><br>

![README](../04-images/M03-17.png)  

<br><br>

<hr>




## Unit 6: Execute the Dataform pipeline 

Follow the screenshots below to execute the pipeline, and see the results in Dataform.

![README](../04-images/M03-18a.png)  

<br><br>

![README](../04-images/M03-18b.png)  

<br><br>

![README](../04-images/M03-18c.png)  

<br><br>

![README](../04-images/M03-19.png)  

<br><br>
![README](../04-images/M03-20.png)  

<br><br>

![README](../04-images/M03-21.png)  

<br><br>

![README](../04-images/M03-22.png)  

<br><br>

<hr>



## Unit 7: Validate the warehouse schema, tables and data loaded

Follow the screenshots below to review the schema, tables generated and the data loaded.

### 7.1. Tables generated

![README](../04-images/M03-23.png)  

<br><br>

![README](../04-images/M03-24.png)  

<br><br>

<hr>

### 7.2. Lineage

![README](../04-images/M03-25.png)  

<br><br>

<hr>

### 7.3. Browse data in a few tables

![README](../04-images/M03-26.png)  

<br><br>

![README](../04-images/M03-27.png)  

<br><br>

![README](../04-images/M03-28.png)  

<br><br>

<hr>


### This concludes Lab BigQuery Data Engineering Agent for Data Warehousing. Proceed to the [next module](Module-04-Data-Engineering-Agent-For-Reporting.md) where we will run analytics using the Data Engineering Agent.

<hr>






















