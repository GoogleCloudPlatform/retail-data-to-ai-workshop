{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "_7I4iD7gvF8a"
      },
      "id": "_7I4iD7gvF8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 04: BigQuery Data Engineering Agent for Reporting Mart generation\n",
        "\n",
        "This notebook contains the prep needed to build the reporting mart off of the code generated by the BigQuery Data Engineering Agent.\n",
        "\n",
        "Programmatic invocation of the Data Engineering Agent is currently not supported, we will therefore do the pre-work here and switch to the UI to generate the Reporting Mart code"
      ],
      "metadata": {
        "id": "U4WdsTx_KE_L"
      },
      "id": "U4WdsTx_KE_L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Foundations"
      ],
      "metadata": {
        "id": "YhJfif4x8ODL"
      },
      "id": "YhJfif4x8ODL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Installs"
      ],
      "metadata": {
        "id": "dXNJGFFZKyY7"
      },
      "id": "dXNJGFFZKyY7"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-dataplex==2.11.0  -q\n",
        "!pip install google-cloud-dataform==0.6.2 -q"
      ],
      "metadata": {
        "id": "jr1g1kZxVllG",
        "collapsed": true
      },
      "id": "jr1g1kZxVllG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Variable initialization & imports"
      ],
      "metadata": {
        "id": "tatq0JIF8pya"
      },
      "id": "tatq0JIF8pya"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID_LIST=!gcloud config list --format \"value(core.project)\" 2>/dev/null\n",
        "PROJECT_ID=PROJECT_ID_LIST[0]\n",
        "LOCATION=\"us-central1\"\n",
        "OLTP_DATASET_ID=\"rscw_oltp_stg_ds\"\n",
        "DWH_DATASET_ID=\"rscw_dwh_ds\"\n",
        "OLTP_METADATA_DATASET_ID=\"rscw_oltp_metadata_ds\"\n",
        "DWH_METADATA_DATASET_ID=\"rscw_dwh_metadata_ds\"\n",
        "DATAFORM_REPO=\"rscw-df-repo\"\n",
        "DATAFORM_WORKSPACE=\"rscw-df-ws\"\n",
        "OLTP_DATASET_RESOURCE_URI = f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{OLTP_DATASET_ID}\"\n",
        "DWH_DATASET_RESOURCE_URI = f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{DWH_DATASET_ID}\"\n",
        "BASE_URL_FOR_DATAPLEX_SCAN=\"https://dataplex.googleapis.com/v1\"\n",
        "MAX_POLLS=250\n",
        "POLLING_INTERVAL_SECONDS=5"
      ],
      "metadata": {
        "id": "A25x8VbPO6tz"
      },
      "id": "A25x8VbPO6tz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "import requests\n",
        "import datetime\n",
        "import decimal\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import psycopg2\n",
        "import google.genai\n",
        "import git\n",
        "import traceback\n",
        "import subprocess\n",
        "import shutil\n",
        "import looker_sdk\n",
        "import configparser\n",
        "\n",
        "from typing import List, Dict, Any, Optional\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import dataplex_v1\n",
        "from google.cloud import dataform_v1beta1\n",
        "from google.api_core import exceptions, operation\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.cloud.exceptions import Conflict\n",
        "from google.protobuf.timestamp_pb2 import Timestamp\n",
        "from google.genai.types import CreateBatchJobConfig, JobState\n",
        "from google.cloud import storage\n",
        "from google.cloud import secretmanager\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import ToolContext"
      ],
      "metadata": {
        "id": "pkIaCNCjWPgD",
        "collapsed": true
      },
      "id": "pkIaCNCjWPgD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Util functions"
      ],
      "metadata": {
        "id": "HaOuahMBX2hK"
      },
      "id": "HaOuahMBX2hK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. General functions"
      ],
      "metadata": {
        "id": "6UaX--K6LNOw"
      },
      "id": "6UaX--K6LNOw"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_auth_token():\n",
        "    creds, _ = google.auth.default()\n",
        "    # Refresh the credentials to get an access token\n",
        "    creds.refresh(google.auth.transport.requests.Request())\n",
        "    return creds.token\n",
        "\n",
        "def sanitize_string_with_hyphens(input_string):\n",
        "    \"\"\"\n",
        "    Converts a string to lowercase and replaces any character that is not\n",
        "    a lowercase letter or a number with a hyphen.\n",
        "    \"\"\"\n",
        "    # convert the entire string to lowercase.\n",
        "    processed_string = input_string.lower()\n",
        "\n",
        "    # The pattern [^a-z0-9] matches any single character that is NOT\n",
        "    # a lowercase letter (a-z) or a digit (0-9).\n",
        "    sanitized_string = re.sub(r'[^a-z0-9]', '-', processed_string)\n",
        "\n",
        "    return sanitized_string\n"
      ],
      "metadata": {
        "id": "iiAeBPxeX0SF"
      },
      "id": "iiAeBPxeX0SF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. BQ utils"
      ],
      "metadata": {
        "id": "ko5Ysz0ddz35"
      },
      "id": "ko5Ysz0ddz35"
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_bigquery_table(bq_table_uri: str):\n",
        "    \"\"\"\n",
        "    Deletes all rows from a specified BigQuery table.\n",
        "\n",
        "    This function executes a TRUNCATE TABLE DML statement, which is an\n",
        "    efficient way to clear a table.\n",
        "\n",
        "    Returns:\n",
        "        A string confirming the successful truncation or describing an error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construct a BigQuery client object.\n",
        "        client = bigquery.Client()\n",
        "\n",
        "        # Sanitize the table URI by wrapping it in backticks.\n",
        "        safe_table_uri = f\"`{bq_table_uri}`\"\n",
        "\n",
        "        # Construct the DML query to truncate the table.\n",
        "        truncate_query = f\"TRUNCATE TABLE {safe_table_uri}\"\n",
        "\n",
        "        # Execute the query.\n",
        "        print(f\"Executing query: {truncate_query}\")\n",
        "        query_job = client.query(truncate_query)\n",
        "\n",
        "        # Wait for the DML query to complete.\n",
        "        query_job.result()\n",
        "\n",
        "        return f\"Successfully truncated table {bq_table_uri}\"\n",
        "\n",
        "    except exceptions.NotFound:\n",
        "        return f\"Error: The table '{bq_table_uri}' was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "def read_bigquery_table(\n",
        "    project_id: str,\n",
        "    dataset_id: str,\n",
        "    table_id: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads all rows from a BigQuery table and returns them as a pandas DataFrame.\n",
        "\n",
        "    This function authenticates using the environment's default credentials and uses\n",
        "    the BigQuery Storage Read API for efficient data retrieval.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing all rows from the table.\n",
        "        Returns an empty DataFrame if the table is not found or an error occurs.\n",
        "    \"\"\"\n",
        "    # Construct a BigQuery client object.\n",
        "    # The client library will automatically handle authentication.\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # Construct the full table ID in the format `project.dataset.table`.\n",
        "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "    try:\n",
        "        # Use the list_rows() method to get a row iterator from the API.\n",
        "        # The to_dataframe() method downloads all rows and converts them to a DataFrame.\n",
        "        print(f\"Reading all rows from table: {table_ref}...\")\n",
        "        rows = client.list_rows(table_ref)\n",
        "        dataframe = rows.to_dataframe()\n",
        "        print(f\"Successfully read {len(dataframe)} rows.\")\n",
        "        return dataframe\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        # Return an empty DataFrame in case of an error.\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def write_dict_to_bigquery(bq_table_uri: str, data_to_insert: dict, delete_conditions: dict = None):\n",
        "    \"\"\"\n",
        "    Writes a row of data to a BigQuery table.\n",
        "\n",
        "    Optionally deletes rows from the table based on a condition before inserting new data.\n",
        "    Create the dataset if it doesn't exist.\n",
        "    Create the table if it doesn't exist, using the data's keys as the schema.\n",
        "    Truncate the table if it exists.\n",
        "    Update the table schema if the data contains new columns.\n",
        "    Insert the data as a new row.\n",
        "\n",
        "    Returns:\n",
        "        A string indicating success or failure.\n",
        "    \"\"\"\n",
        "    if not data_to_insert:\n",
        "        return \"No data provided to write to BigQuery.\"\n",
        "\n",
        "    try:\n",
        "        # Construct a BigQuery client object.\n",
        "        client = bigquery.Client()\n",
        "\n",
        "        # Parse the table URI.\n",
        "        project_id, dataset_id, table_id = bq_table_uri.split('.')\n",
        "        dataset_ref = client.dataset(dataset_id)\n",
        "        table_ref = dataset_ref.table(table_id)\n",
        "        table_ref_str = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "        # Create dataset if it doesn't exist.\n",
        "        try:\n",
        "            client.get_dataset(dataset_ref)\n",
        "        except exceptions.NotFound:\n",
        "            print(f\"Dataset {dataset_id} not found. Creating it in us-central1.\")\n",
        "            dataset = bigquery.Dataset(dataset_ref)\n",
        "            dataset.location = \"us-central1\"\n",
        "            client.create_dataset(dataset, exists_ok=True)\n",
        "\n",
        "        # Prepare the schema based on the data to insert.\n",
        "        new_schema_fields = [\n",
        "            bigquery.SchemaField(key, \"STRING\") for key in data_to_insert\n",
        "        ]\n",
        "\n",
        "        # Check if table exists and update schema if necessary.\n",
        "        try:\n",
        "            table = client.get_table(table_ref)\n",
        "            current_schema = {field.name for field in table.schema}\n",
        "            new_schema = list(table.schema)\n",
        "\n",
        "            for field in new_schema_fields:\n",
        "                if field.name not in current_schema:\n",
        "                    new_schema.append(field)\n",
        "\n",
        "            table.schema = new_schema\n",
        "            client.update_table(table, [\"schema\"])\n",
        "\n",
        "        except exceptions.NotFound:\n",
        "            print(f\"Table {table_id} not found. Creating it.\")\n",
        "            table = bigquery.Table(table_ref_str, schema=new_schema_fields)\n",
        "            client.create_table(table)\n",
        "\n",
        "        # Delete rows if conditions are provided.\n",
        "        if delete_conditions and table_id in [\"tables\", \"columns\"]:\n",
        "            where_clauses = []\n",
        "            for condition in delete_conditions:\n",
        "                column = condition.get('column')\n",
        "                value = condition.get('value')\n",
        "                if column and value is not None:\n",
        "                    where_clauses.append(f\"{column} = '{value}'\")\n",
        "\n",
        "            if where_clauses:\n",
        "                delete_query = f\"DELETE FROM {table_ref_str} WHERE \" + \" AND \".join(where_clauses)\n",
        "                query_job = client.query(delete_query)\n",
        "                query_job.result()  # Wait for the job to complete.\n",
        "\n",
        "        # Insert the new row.\n",
        "        row_to_insert = {key: str(value) for key, value in data_to_insert.items()}\n",
        "        errors = client.insert_rows_json(table_ref_str, [row_to_insert])\n",
        "\n",
        "        if not errors:\n",
        "            return f\"Successfully wrote data to {table_ref_str}\"\n",
        "        else:\n",
        "            return f\"Failed to insert rows: {errors}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "def update_bigquery_metadata(\n",
        "    project_id: str,\n",
        "    dataset_id: str,\n",
        "    new_description: str,\n",
        "    table_id: str = None,\n",
        "    column_name: str = None\n",
        "):\n",
        "  \"\"\"\n",
        "  Updates the description of a BigQuery dataset, table, or column.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    if table_id and column_name:\n",
        "      # Update a column's description\n",
        "      dataset_ref = client.dataset(dataset_id)\n",
        "      table_ref = dataset_ref.table(table_id)\n",
        "      table = client.get_table(table_ref)\n",
        "\n",
        "      new_schema = []\n",
        "      column_found = False\n",
        "      for field in table.schema:\n",
        "        if field.name == column_name:\n",
        "          column_found = True\n",
        "          # Recreate the SchemaField with the new description\n",
        "          new_field = field.to_api_repr()\n",
        "          new_field['description'] = new_description\n",
        "          new_schema.append(bigquery.SchemaField.from_api_repr(new_field))\n",
        "        else:\n",
        "          new_schema.append(field)\n",
        "\n",
        "      if not column_found:\n",
        "        print(f\"Error: Column '{column_name}' not found in table '{table_id}'.\")\n",
        "        return\n",
        "\n",
        "      table.schema = new_schema\n",
        "      client.update_table(table, [\"schema\"])\n",
        "      print(f\"Successfully updated description for column: {column_name} in table {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    elif table_id:\n",
        "      # Update a table's description\n",
        "      dataset_ref = client.dataset(dataset_id)\n",
        "      table_ref = dataset_ref.table(table_id)\n",
        "      table = client.get_table(table_ref)\n",
        "      table.description = new_description\n",
        "      client.update_table(table, [\"description\"])\n",
        "      print(f\"Successfully updated description for table: {project_id}.{dataset_id}.{table_id}\")\n",
        "\n",
        "    else:\n",
        "      # Update a dataset's description\n",
        "      dataset_ref = client.dataset(dataset_id)\n",
        "      dataset = client.get_dataset(dataset_ref)\n",
        "      dataset.description = new_description\n",
        "      client.update_dataset(dataset, [\"description\"])\n",
        "      print(f\"Successfully updated description for dataset: {project_id}.{dataset_id}\")\n",
        "\n",
        "  except NotFound as e:\n",
        "    print(f\"Error: Resource not found. Please check your IDs. Details: {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "def get_dataset_tables(dataset_id):\n",
        "  \"\"\"\n",
        "  Fetches a list of tables within a specified BigQuery dataset.\n",
        "  This function initializes a BigQuery client for a predefined project\n",
        "  and retrieves an iterator for the tables in the given dataset.\n",
        "  \"\"\"\n",
        "  client = bigquery.Client(project=f\"{PROJECT_ID}\")\n",
        "  tables = client.list_tables(dataset_id)\n",
        "  return tables"
      ],
      "metadata": {
        "id": "zNaFRCOXd1VL"
      },
      "id": "zNaFRCOXd1VL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Dataform utils"
      ],
      "metadata": {
        "id": "56uZ-UyyX6ZX"
      },
      "id": "56uZ-UyyX6ZX"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def delete_dataform_workspace(token: str, project_id: str, location: str, repository_id: str, workspace_id: str):\n",
        "    \"\"\"\n",
        "    Deletes a Dataform workspace.\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "    }\n",
        "    workspace_url = f\"https://dataform.googleapis.com/v1beta1/projects/{project_id}/locations/{location}/repositories/{repository_id}/workspaces/{workspace_id}\"\n",
        "\n",
        "    print(f\"Attempting to delete workspace '{workspace_id}'...\")\n",
        "    try:\n",
        "        response = requests.delete(workspace_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        print(f\"Workspace '{workspace_id}' deleted successfully.\")\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        if e.response.status_code == 404:\n",
        "            print(f\"Workspace '{workspace_id}' not found.\")\n",
        "        else:\n",
        "            print(f\"Failed to delete workspace: {e.response.text}\")\n",
        "\n",
        "def create_dataform_repository_and_workspace(token, project_id, location, repository_id, workspace_id):\n",
        "    \"\"\"\n",
        "    Checks for a Dataform repository, creates it if it doesn't exist,\n",
        "    and then creates a workspace in it.\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    repo_url = f\"https://dataform.googleapis.com/v1beta1/projects/{project_id}/locations/{location}/repositories/{repository_id}\"\n",
        "\n",
        "    # Check if the repository exists\n",
        "    repo_check_response = requests.get(repo_url, headers=headers)\n",
        "\n",
        "    repo_exists = False\n",
        "    if repo_check_response.status_code == 200:\n",
        "        print(f\"Repository '{repository_id}' already exists.\")\n",
        "        repo_exists = True\n",
        "    elif repo_check_response.status_code == 404:\n",
        "        print(f\"Repository '{repository_id}' not found. Creating it...\")\n",
        "        # Create the repository since it doesn't exist\n",
        "        create_repo_url = f\"https://dataform.googleapis.com/v1beta1/projects/{project_id}/locations/{location}/repositories?repositoryId={repository_id}\"\n",
        "        repo_create_response = requests.post(create_repo_url, headers=headers)\n",
        "\n",
        "        if repo_create_response.status_code == 200:\n",
        "            print(f\"Repository '{repository_id}' created successfully.\")\n",
        "            repo_exists = True\n",
        "        else:\n",
        "            print(f\"Failed to create repository: {repo_create_response.text}\")\n",
        "    else:\n",
        "        # Handle other potential errors during the check\n",
        "        print(f\"Error checking repository: {repo_check_response.text}\")\n",
        "\n",
        "    # If the repository exists (either pre-existing or newly created), create the workspace\n",
        "    if repo_exists:\n",
        "        # Corrected the workspace URL construction\n",
        "        workspace_url = f\"{repo_url}/workspaces?workspaceId={workspace_id}\"\n",
        "        workspace_response = requests.post(workspace_url, headers=headers)\n",
        "\n",
        "        if workspace_response.status_code == 200:\n",
        "            print(f\"Workspace '{workspace_id}' created successfully.\")\n",
        "        elif workspace_response.status_code == 409:\n",
        "             print(f\"Workspace '{workspace_id}' already exists.\")\n",
        "        else:\n",
        "            print(f\"Failed to create workspace: {workspace_response.text}\")\n",
        "\n",
        "\n",
        "\n",
        "def initialize_dataform_workspace(token, project_id, location, repository_id, workspace_id):\n",
        "    \"\"\"\n",
        "    Initializes a Dataform workspace by creating initial files and installing packages.\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    workspace_path = f\"projects/{project_id}/locations/{location}/repositories/{repository_id}/workspaces/{workspace_id}\"\n",
        "    base_url = f\"https://dataform.googleapis.com/v1beta1/{workspace_path}\"\n",
        "\n",
        "    # 1. Create workflow_settings.yaml\n",
        "    workflow_settings = {\n",
        "        'defaultProject': project_id,\n",
        "        'defaultLocation': location,\n",
        "        'defaultDataset': 'dataform',\n",
        "        'defaultAssertionDataset': 'dataform_assertions',\n",
        "        'dataformCoreVersion': '3.0.16',\n",
        "    }\n",
        "    yaml_string = yaml.dump(workflow_settings)\n",
        "    # The API expects the file content to be a base64 encoded string\n",
        "    encoded_yaml = base64.b64encode(yaml_string.encode('utf-8')).decode('utf-8')\n",
        "\n",
        "    write_file_url = f\"{base_url}:writeFile\"\n",
        "    write_settings_payload = {\n",
        "        \"path\": \"workflow_settings.yaml\",\n",
        "        \"contents\": encoded_yaml\n",
        "    }\n",
        "\n",
        "    settings_response = requests.post(write_file_url, headers=headers, json=write_settings_payload)\n",
        "\n",
        "    if settings_response.status_code == 200:\n",
        "        print(\"Successfully wrote workflow_settings.yaml.\")\n",
        "    else:\n",
        "        print(f\"Failed to write workflow_settings.yaml: {settings_response.text}\")\n",
        "        return\n",
        "\n",
        "    # 2. Create .gitignore\n",
        "    gitignore_content = \"node_modules/\"\n",
        "    encoded_gitignore = base64.b64encode(gitignore_content.encode('utf-8')).decode('utf-8')\n",
        "    write_gitignore_payload = {\n",
        "        \"path\": \".gitignore\",\n",
        "        \"contents\": encoded_gitignore\n",
        "    }\n",
        "    gitignore_response = requests.post(write_file_url, headers=headers, json=write_gitignore_payload)\n",
        "\n",
        "    if gitignore_response.status_code == 200:\n",
        "        print(\"Successfully wrote .gitignore.\")\n",
        "    else:\n",
        "        print(f\"Failed to write .gitignore: {gitignore_response.text}\")\n",
        "        return\n",
        "\n",
        "    # 3. Install npm packages\n",
        "    print(\"Installing npm packages...\")\n",
        "    install_packages_url = f\"{base_url}:installNpmPackages\"\n",
        "    packages_response = requests.post(install_packages_url, headers=headers)\n",
        "\n",
        "    if packages_response.status_code == 200:\n",
        "        print(\"NPM packages installed successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to install NPM packages: {packages_response.text}\")\n",
        "\n",
        "def get_dataform_workspace_contents(\n",
        "    token: str,\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    repository_id: str,\n",
        "    workspace_id: str,\n",
        "    dataform_dir: str = \"definitions\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Lists files in a Dataform workspace's 'definitions' folder, reads their content,\n",
        "    and provides an explanation.\n",
        "\n",
        "    This function recursively lists all files within the 'definitions' directory,\n",
        "    reads the content of each one, decodes it, and generates a high-level summary\n",
        "    of what the SQLX file does based on its configuration and query.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary contains the file path, its\n",
        "        decoded content, and an explanation. Returns None on failure.\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"Querying Dataform workspace files in '{dataform_dir}/' folder...\")\n",
        "        all_files = list_all_dataform_files(token, dataform_dir)\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"API Error while listing files: {e}\\nResponse body: {e.response.text}\")\n",
        "        return None\n",
        "\n",
        "    # Read the content of each file\n",
        "    file_contents = []\n",
        "    read_endpoint_base = (\n",
        "        f\"https://dataform.googleapis.com/v1/projects/{project_id}/\"\n",
        "        f\"locations/{location}/repositories/{repository_id}/workspaces/{workspace_id}:readFile\"\n",
        "    )\n",
        "\n",
        "    for file_path in all_files:\n",
        "        try:\n",
        "            response = requests.get(read_endpoint_base, headers=headers, params={\"path\": file_path})\n",
        "            response.raise_for_status()\n",
        "\n",
        "            encoded_content = response.json().get(\"fileContents\", \"\")\n",
        "            decoded_content = base64.b64decode(encoded_content).decode(\"utf-8\")\n",
        "\n",
        "            explanation = \"Could not determine the purpose of this file.\"\n",
        "            if file_path.endswith(\".sqlx\"):\n",
        "                config_type_match = re.search(r'type:\\s*[\"\\'](\\w+)[\"\\']', decoded_content)\n",
        "                action_type = config_type_match.group(1) if config_type_match else \"action\"\n",
        "                refs = re.findall(r\"ref\\(['\\\"]([\\w_]+)['\\\"]\\)\", decoded_content)\n",
        "\n",
        "                if refs:\n",
        "                    explanation = f\"This file defines a new {action_type} that depends on the following source(s): {', '.join(refs)}.\"\n",
        "                else:\n",
        "                    explanation = f\"This file defines a new {action_type}.\"\n",
        "\n",
        "            file_contents.append({\n",
        "                \"file_path\": file_path,\n",
        "                \"content\": decoded_content,\n",
        "                \"explanation\": explanation\n",
        "            })\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            print(f\"API Error reading file '{file_path}': {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred processing file '{file_path}': {e}\")\n",
        "            continue\n",
        "\n",
        "    return file_contents\n",
        "\n",
        "def execute_dataform_pipeline(\n",
        "    token: str,\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    repository_id: str,\n",
        "    workspace_id: str,\n",
        "    service_account_email: str,\n",
        "    target_table_name: str = \"\"\n",
        ")-> dict:\n",
        "    \"\"\"\n",
        "    Compiles a Dataform workspace, executes the pipeline, and waits for it to complete.\n",
        "\n",
        "    If a target_table_name is provided, only actions tagged with that name will be executed.\n",
        "\n",
        "    Returns:\n",
        "        The final workflow invocation object, or None if any step fails.\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Compile the workspace ---\n",
        "    print(\"Compiling Dataform workspace...\")\n",
        "    compilation_endpoint = (\n",
        "        f\"https://dataform.googleapis.com/v1/projects/{project_id}/\"\n",
        "        f\"locations/{location}/repositories/{repository_id}/compilationResults\"\n",
        "    )\n",
        "    workspace_resource_name = (\n",
        "        f\"projects/{project_id}/locations/{location}/\"\n",
        "        f\"repositories/{repository_id}/workspaces/{workspace_id}\"\n",
        "    )\n",
        "    compilation_body = {\"workspace\": workspace_resource_name}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(compilation_endpoint, headers=headers, json=compilation_body)\n",
        "        response.raise_for_status()\n",
        "        compilation_result = response.json()\n",
        "        compilation_result_name = compilation_result.get(\"name\")\n",
        "        print(f\"Successfully compiled. Result name: {compilation_result_name}\")\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"API Error during compilation: {e}\\nResponse body: {e.response.text}\")\n",
        "        return None\n",
        "\n",
        "    # --- Step 2: Execute the compiled result ---\n",
        "    print(\"Executing the pipeline...\")\n",
        "    invocation_endpoint = (\n",
        "        f\"https://dataform.googleapis.com/v1/projects/{project_id}/\"\n",
        "        f\"locations/{location}/repositories/{repository_id}/workflowInvocations\"\n",
        "    )\n",
        "\n",
        "    invocation_config = {\n",
        "        \"serviceAccount\": service_account_email\n",
        "    }\n",
        "\n",
        "    # If a target_table_name is provided, include it in the invocationConfig to filter execution by tags.\n",
        "    if len(target_table_name) > 0:\n",
        "        invocation_config[\"includedTags\"] = [target_table_name]\n",
        "        print(f\"Executing only actions tagged with: {target_table_name}\")\n",
        "\n",
        "    invocation_body = {\n",
        "        \"compilationResult\": compilation_result_name,\n",
        "        \"invocationConfig\": invocation_config\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(invocation_endpoint, headers=headers, json=invocation_body)\n",
        "        response.raise_for_status()\n",
        "        workflow_invocation = response.json()\n",
        "        invocation_name = workflow_invocation.get('name')\n",
        "        print(f\"Successfully started workflow invocation: {invocation_name}\")\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"API Error during execution: {e}\\nResponse body: {e.response.text}\")\n",
        "        return None\n",
        "\n",
        "    # --- Step 3: Wait for the execution to complete ---\n",
        "    print(\"\\nWaiting for execution to complete...\")\n",
        "    status_endpoint = f\"https://dataform.googleapis.com/v1/{invocation_name}\"\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            status_response = requests.get(status_endpoint, headers=headers)\n",
        "            status_response.raise_for_status()\n",
        "            invocation_details = status_response.json()\n",
        "            current_state = invocation_details.get(\"state\")\n",
        "\n",
        "            print(f\"Current state: {current_state}\")\n",
        "\n",
        "            if current_state in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n",
        "                print(f\"Execution finished with state: {current_state}\")\n",
        "                if current_state == \"SUCCEEDED\":\n",
        "                     print(\"Pipeline executed successfully.\")\n",
        "                else:\n",
        "                     print(\"Pipeline execution did not succeed.\")\n",
        "                return invocation_details\n",
        "\n",
        "            # Wait for 2 seconds before checking the status again\n",
        "            time.sleep(2)\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            print(f\"API Error while checking status: {e}\\nResponse body: {e.response.text}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def get_executed_files_from_invocation(token: str, invocation_details: dict) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extracts the list of executed file paths from a Dataform workflow invocation object.\n",
        "    This version includes detailed print statements for debugging.\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    compilation_result_name = invocation_details.get(\"compilationResult\")\n",
        "    if not compilation_result_name:\n",
        "        print(\"No compilationResult found in invocation details.\")\n",
        "        return []\n",
        "\n",
        "    compilation_result_url = f\"https://dataform.googleapis.com/v1/{compilation_result_name}\"\n",
        "\n",
        "    try:\n",
        "        print(f\"Fetching compilation result from: {compilation_result_url}\")\n",
        "        response = requests.get(compilation_result_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        compilation_result = response.json()\n",
        "\n",
        "        # --- New: Print the full compilation result for inspection ---\n",
        "        print(\"\\n--- Full Compilation Result ---\")\n",
        "        print(json.dumps(compilation_result, indent=2))\n",
        "        print(\"-------------------------------\\n\")\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"API Error during compilation result fetch: {e}\\nResponse body: {e.response.text}\")\n",
        "        return []\n",
        "\n",
        "    executed_files = []\n",
        "    included_tags = invocation_details.get(\"invocationConfig\", {}).get(\"includedTags\", [])\n",
        "    print(f\"Execution was filtered by the following tags: {included_tags}\")\n",
        "\n",
        "    if \"compilationResultActions\" in compilation_result:\n",
        "        print(\"\\n--- Analyzing Compiled Actions ---\")\n",
        "        for action in compilation_result[\"compilationResultActions\"]:\n",
        "            file_path = action.get(\"filePath\", \"N/A\")\n",
        "            action_tags = action.get(\"tags\", [])\n",
        "\n",
        "            print(f\"Found action for file: {file_path} with tags: {action_tags}\")\n",
        "\n",
        "            if not included_tags or any(tag in action_tags for tag in included_tags):\n",
        "                if \"filePath\" in action:\n",
        "                    print(f\"  -> Match found! Adding '{file_path}' to the list of executed files.\")\n",
        "                    executed_files.append(action[\"filePath\"])\n",
        "        print(\"--------------------------------\\n\")\n",
        "    else:\n",
        "        print(\"Warning: 'compilationResultActions' not found in the compilation result.\")\n",
        "\n",
        "\n",
        "    return executed_files\n",
        "\n",
        "def execute_pipeline_code(target_tag_value: str) -> dict:\n",
        "    \"\"\"\n",
        "    Executes the full Dataform pipeline to create whatever is defined in the\n",
        "    Workspace files.\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate the 'dataset' parameter to ensure it's a valid choice.\n",
        "    valid_target_tag_values = [\"reports\", \"\"]\n",
        "    if target_tag_value not in valid_target_tag_values:\n",
        "        raise ValueError(f\"Invalid dataset '{target_tag_value}' provided. Please use one of {valid_target_tag_values}.\")\n",
        "\n",
        "\n",
        "    print(\"--- Tool: Executing the Dataform pipeline... ---\")\n",
        "\n",
        "    token = get_auth_token()\n",
        "    details = {}\n",
        "    # Execute the pipeline from the workspace\n",
        "    final_invocation_details = execute_dataform_pipeline(\n",
        "        token,\n",
        "        PROJECT_ID,\n",
        "        LOCATION,\n",
        "        DATAFORM_REPO,\n",
        "        DATAFORM_WORKSPACE,\n",
        "        SERVICE_ACCOUNT_FOR_DATAFORM,\n",
        "        target_tag_value\n",
        "    )\n",
        "\n",
        "    if final_invocation_details:\n",
        "        print(\"\\n--- Fetching executed files ---\")\n",
        "\n",
        "        dataform_directory = \"definitions\"\n",
        "\n",
        "        if target_tag_value in [\"reports\"]:\n",
        "            dataform_directory = \"definitions/reports\"\n",
        "\n",
        "        executed_files = list_all_dataform_files(token, dataform_directory)\n",
        "\n",
        "        details[\"invocation_details\"]=final_invocation_details\n",
        "\n",
        "        if executed_files:\n",
        "            details[\"executed_files\"]=executed_files\n",
        "            print(\"The following files were executed:\")\n",
        "            for file_path in executed_files:\n",
        "                print(f\"- {file_path}\")\n",
        "        else:\n",
        "            print(\"Could not retrieve the list of executed files.\")\n",
        "\n",
        "        print(\"\\n--- Final Invocation Details ---\")\n",
        "        print(json.dumps(final_invocation_details, indent=2))\n",
        "        print(\"------------------------------\")\n",
        "\n",
        "    return {\"status\": \"success\", \"Response\": final_invocation_details, \"Details\":executed_files}\n",
        "\n",
        "def explain_pipeline_code() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Gets the content and explanations of all the files in the Dataform repo\n",
        "    definitions folder and return them as a dictionary.\n",
        "    \"\"\"\n",
        "    print(\"--- Tool: Getting explanations for the files in the workspace... ---\")\n",
        "\n",
        "    token = get_auth_token()\n",
        "\n",
        "    workspace_contents = get_dataform_workspace_contents(\n",
        "        token,\n",
        "        PROJECT_ID,\n",
        "        LOCATION,\n",
        "        DATAFORM_REPO,\n",
        "        DATAFORM_WORKSPACE\n",
        "    )\n",
        "\n",
        "    if workspace_contents:\n",
        "        return {\"status\": \"success\", \"Details\": workspace_contents}\n",
        "    else:\n",
        "        print(\"Could not retrieve the workspace contents.\")\n",
        "        # Return a dictionary indicating failure and an empty list\n",
        "        return {\"status\": \"error\", \"An error occured. Workspace contents are empty\": []}\n",
        "\n",
        "def list_all_dataform_files(token: str, dataform_dir: str = \"definitions\"):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    query_endpoint = (\n",
        "        f\"https://dataform.googleapis.com/v1/projects/{PROJECT_ID}/\"\n",
        "        f\"locations/{LOCATION}/repositories/{DATAFORM_REPO}/workspaces/{DATAFORM_WORKSPACE}:queryDirectoryContents\"\n",
        "    )\n",
        "\n",
        "    all_files = set()\n",
        "\n",
        "    dirs_to_query = [dataform_dir]\n",
        "    known_dirs = {dataform_dir}\n",
        "\n",
        "    while dirs_to_query:\n",
        "        path_to_query = dirs_to_query.pop(0)\n",
        "\n",
        "        params = {\"path\": path_to_query}\n",
        "        response = requests.get(query_endpoint, headers=headers, params=params)\n",
        "\n",
        "        if response.status_code == 404:\n",
        "            print(f\"Warning: Directory not found at path: '{path_to_query}'. Skipping.\")\n",
        "            continue\n",
        "        response.raise_for_status()\n",
        "\n",
        "        for entry in response.json().get(\"directoryEntries\", []):\n",
        "            if \"directory\" in entry:\n",
        "                full_path = entry['directory']\n",
        "                if full_path not in known_dirs:\n",
        "                    dirs_to_query.append(full_path)\n",
        "                    known_dirs.add(full_path)\n",
        "            elif \"file\" in entry:\n",
        "                full_path = entry['file']\n",
        "                all_files.add(full_path)\n",
        "\n",
        "    return sorted(list(all_files))\n",
        "\n",
        "def get_dataform_workspace_contents(\n",
        "    token: str,\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    repository_id: str,\n",
        "    workspace_id: str,\n",
        "    dataform_dir: str = \"definitions\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Lists files in a Dataform workspace's 'definitions' folder, reads their content,\n",
        "    and provides an explanation.\n",
        "\n",
        "    This function recursively lists all files within the 'definitions' directory,\n",
        "    reads the content of each one, decodes it, and generates a high-level summary\n",
        "    of what the SQLX file does based on its configuration and query.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary contains the file path, its\n",
        "        decoded content, and an explanation. Returns None on failure.\n",
        "    \"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"Querying Dataform workspace files in '{dataform_dir}/' folder...\")\n",
        "        all_files = list_all_dataform_files(token, dataform_dir)\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"API Error while listing files: {e}\\nResponse body: {e.response.text}\")\n",
        "        return None\n",
        "\n",
        "    # Read the content of each file\n",
        "    file_contents = []\n",
        "    read_endpoint_base = (\n",
        "        f\"https://dataform.googleapis.com/v1/projects/{project_id}/\"\n",
        "        f\"locations/{location}/repositories/{repository_id}/workspaces/{workspace_id}:readFile\"\n",
        "    )\n",
        "\n",
        "    for file_path in all_files:\n",
        "        try:\n",
        "            response = requests.get(read_endpoint_base, headers=headers, params={\"path\": file_path})\n",
        "            response.raise_for_status()\n",
        "\n",
        "            encoded_content = response.json().get(\"fileContents\", \"\")\n",
        "            decoded_content = base64.b64decode(encoded_content).decode(\"utf-8\")\n",
        "\n",
        "            explanation = \"Could not determine the purpose of this file.\"\n",
        "            if file_path.endswith(\".sqlx\"):\n",
        "                config_type_match = re.search(r'type:\\s*[\"\\'](\\w+)[\"\\']', decoded_content)\n",
        "                action_type = config_type_match.group(1) if config_type_match else \"action\"\n",
        "                refs = re.findall(r\"ref\\(['\\\"]([\\w_]+)['\\\"]\\)\", decoded_content)\n",
        "\n",
        "                if refs:\n",
        "                    explanation = f\"This file defines a new {action_type} that depends on the following source(s): {', '.join(refs)}.\"\n",
        "                else:\n",
        "                    explanation = f\"This file defines a new {action_type}.\"\n",
        "\n",
        "            file_contents.append({\n",
        "                \"file_path\": file_path,\n",
        "                \"content\": decoded_content,\n",
        "                \"explanation\": explanation\n",
        "            })\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            print(f\"API Error reading file '{file_path}': {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred processing file '{file_path}': {e}\")\n",
        "            continue\n",
        "\n",
        "    return file_contents\n",
        "\n",
        "def delete_dataform_folder(\n",
        "    token: str,\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    repository_id: str,\n",
        "    workspace_id: str,\n",
        "    folder: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Deletes a Dataform folder only if it exists.\n",
        "    \"\"\"\n",
        "\n",
        "    if folder is None:\n",
        "        raise ValueError(\"The 'folder' parameter cannot be None.\")\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    instruction_path = \".gdeagent/instructions\"\n",
        "\n",
        "    # --- Step 1: Check if the directory exists before trying to delete it ---\n",
        "    #queryDirectoryContents will return a 404 if the path does not exist.\n",
        "    check_endpoint = (\n",
        "        f\"https://dataform.googleapis.com/v1beta1/projects/{project_id}/\"\n",
        "        f\"locations/{location}/repositories/{repository_id}/workspaces/{workspace_id}:queryDirectoryContents\"\n",
        "    )\n",
        "\n",
        "    print(f\"Checking for existence of directory: '{folder}'...\")\n",
        "    try:\n",
        "        check_response = requests.get(check_endpoint, headers=headers, params={\"path\": folder})\n",
        "\n",
        "        # A 404 status code means the directory was not found.\n",
        "        if check_response.status_code == 404:\n",
        "            print(f\"Directory '{folder}' not found. Nothing to delete.\")\n",
        "            return  # Exit the function gracefully\n",
        "\n",
        "        # Raise an exception for any other HTTP errors during the check.\n",
        "        check_response.raise_for_status()\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"An error occurred while checking for the directory: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- Step 2: If the check was successful, the directory exists, so we delete it ---\n",
        "    print(f\"Directory '{folder}' found. Proceeding with deletion...\")\n",
        "    delete_endpoint = (\n",
        "        f\"https://dataform.googleapis.com/v1beta1/projects/{project_id}/\"\n",
        "        f\"locations/{location}/repositories/{repository_id}/workspaces/{workspace_id}:removeDirectory\"\n",
        "    )\n",
        "    body = {\"path\": folder}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(delete_endpoint, headers=headers, json=body)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes during deletion\n",
        "        print(f\"Successfully deleted directory: '{folder}'\")\n",
        "        return response.json()\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"An error occurred during deletion: {e}\")\n",
        "        # Re-raise the exception to be handled by the caller if deletion fails\n",
        "        raise e\n",
        "\n",
        "def create_dataform_instructions_file(\n",
        "    token: str,\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    repository_id: str,\n",
        "    workspace_id: str,\n",
        "    filename: str,\n",
        "    content: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a Dataform instruction file with the given content.\n",
        "\n",
        "    Returns:\n",
        "        The JSON response from the Dataform API.\n",
        "    \"\"\"\n",
        "\n",
        "    # The Dataform API endpoint for writing a file\n",
        "    # This is based on the v1beta1 API version\n",
        "    api_endpoint = (\n",
        "        f\"https://dataform.googleapis.com/v1beta1/projects/{project_id}/\"\n",
        "        f\"locations/{location}/repositories/{repository_id}/workspaces/{workspace_id}:writeFile\"\n",
        "    )\n",
        "\n",
        "    # Instruction files must be placed in the .gdeagent/instructions/ directory\n",
        "    # No subdirectories are allowed.\n",
        "    instruction_path = f\".gdeagent/instructions/{filename}\"\n",
        "\n",
        "    # The content of the file must be base64 encoded\n",
        "    encoded_content = base64.b64encode(content.encode(\"utf-8\")).decode(\"utf-8\")\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    body = {\n",
        "        \"path\": instruction_path,\n",
        "        \"contents\": encoded_content,\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_endpoint, headers=headers, json=body)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "_Ly3IWscWAnV"
      },
      "id": "_Ly3IWscWAnV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Dataplex utils\n"
      ],
      "metadata": {
        "id": "XcRN91ZqZihI"
      },
      "id": "XcRN91ZqZihI"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_operation_success_status(token, operation_name) -> bool:\n",
        "    \"\"\"\n",
        "    Gets the status of an operation until it is 'done'.\n",
        "    \"\"\"\n",
        "\n",
        "    url = f\"{BASE_URL_FOR_DATAPLEX_SCAN}/{operation_name}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    operation = response.json()\n",
        "\n",
        "    if operation.get(\"done\"):\n",
        "        if operation.get(\"error\"):\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "def generate_patch_label_request_body(scan_type, scan_id):\n",
        "    \"\"\"\n",
        "        Returns the patch labels json that needs to be attached to the source table to tie programmatic scans to the UI\n",
        "\n",
        "        Args:\n",
        "            scan_type (str): Type of scan (DATA_PROFLE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN)\n",
        "            scan_id (str): Scan id\n",
        "            operation_type (str): Type of operation (CREATE_SCAN/RUN_SCAN)\n",
        "\n",
        "        Returns:\n",
        "            string: json with the patch labels\n",
        "        \"\"\"\n",
        "    label_json=\"\"\n",
        "    scan_stub = \"\"\n",
        "    if scan_type == \"DATA_PROFILE_SCAN\":\n",
        "        scan_stub=\"dp\"\n",
        "    elif scan_type == \"DATA_DOCUMENTATION_SCAN\":\n",
        "        scan_stub=\"data-documentation\"\n",
        "    elif scan_type == \"DATA_KNOWLEDGE_ENGINE_SCAN\":\n",
        "        scan_stub=\"data-documentation\"\n",
        "\n",
        "\n",
        "    label_json = {\n",
        "        \"labels\": {f\"dataplex-{scan_stub}-published-scan\":f\"{scan_id}\",\n",
        "                 f\"dataplex-{scan_stub}-published-project\":f\"{PROJECT_ID}\",\n",
        "                 f\"dataplex-{scan_stub}-published-location\":f\"{LOCATION}\"}\n",
        "      }\n",
        "\n",
        "    return label_json\n",
        "\n",
        "def patch_source_table_with_labels(token, dataset, scan_type, scan_nm, source_table_nm=None):\n",
        "    \"\"\"\n",
        "    Patches the source BigQuery table or dataset with labels to correlate with the data scans.\n",
        "\n",
        "    If a table name is provided, it will patch the table. Otherwise, it will patch the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (str): The ID of the dataset.\n",
        "        scan_type (str): The type of the scan.\n",
        "        scan_nm (str): The name of the scan.\n",
        "        source_table_nm (str, optional): The name of the source table. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "\n",
        "    if source_table_nm:\n",
        "        API_ENDPOINT = f\"https://bigquery.googleapis.com/bigquery/v2/projects/{PROJECT_ID}/datasets/{dataset}/tables/{source_table_nm}\"\n",
        "    else:\n",
        "        API_ENDPOINT = f\"https://bigquery.googleapis.com/bigquery/v2/projects/{PROJECT_ID}/datasets/{dataset}\"\n",
        "\n",
        "    patch_request_body = generate_patch_label_request_body(scan_type, scan_nm)\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    # Apply the patch\n",
        "    try:\n",
        "        response = requests.patch(API_ENDPOINT, headers=headers, json=patch_request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "\n",
        "def create_table_data_documentation_scan_job(token, project_id, location, dataset_resource, data_scan_id, table_id, delete_existing=False):\n",
        "    \"\"\"\n",
        "    Initiates the creation of a Dataplex Data Documentation scan.\n",
        "    If delete_existing is True, it first deletes the scan if it already exists.\n",
        "    \"\"\"\n",
        "\n",
        "    url = f\"{BASE_URL_FOR_DATAPLEX_SCAN}/projects/{project_id}/locations/{location}/dataScans?dataScanId={data_scan_id}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
        "    if delete_existing:\n",
        "        print(f\"Attempting to delete existing data documentation scan '{data_scan_id}'...\")\n",
        "        try:\n",
        "            delete_response = requests.delete(url, headers=headers)\n",
        "            delete_response.raise_for_status()\n",
        "            print(f\"Successfully initiated deletion for scan '{data_scan_id}'.\")\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 404:\n",
        "                print(f\"Scan '{data_scan_id}' not found, no need to delete.\")\n",
        "            else:\n",
        "                raise\n",
        "    payload = {\n",
        "        \"displayName\": data_scan_id,\n",
        "        \"data\": {\"resource\": f\"{dataset_resource}/tables/{table_id}\"},\n",
        "        \"executionSpec\": {\"trigger\": {\"onDemand\": {}}},\n",
        "        \"type\": \"DATA_DOCUMENTATION\",\n",
        "        \"dataDocumentationSpec\": {}\n",
        "    }\n",
        "    print(f\"Attempting to create data documentation scan {data_scan_id}...\")\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        operation_name = response.json().get(\"name\")\n",
        "        print(f\"Scan creation initiated. Operation: {operation_name}\")\n",
        "        return operation_name\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        # If a 409 conflict occurs, it means the resource exists, which is acceptable.\n",
        "        if e.response.status_code == 409:\n",
        "            print(f\"Scan '{data_scan_id}' already exists. Skipping creation and proceeding.\")\n",
        "            return None\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def create_dataset_documentation_scan_job(token, project_id, location, dataset_resource, scan_id, delete_existing=False):\n",
        "    \"\"\"\n",
        "    Initiates the creation of a Dataplex Knowledge Engine scan.\n",
        "    If delete_existing is True, it first deletes the scan if it already exists.\n",
        "    \"\"\"\n",
        "\n",
        "    url = f\"{BASE_URL_FOR_DATAPLEX_SCAN}/projects/{project_id}/locations/{location}/dataScans?dataScanId={scan_id}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
        "    if delete_existing:\n",
        "        print(f\"Attempting to delete existing data scan '{scan_id}'...\")\n",
        "        try:\n",
        "            delete_response = requests.delete(url, headers=headers)\n",
        "            delete_response.raise_for_status()\n",
        "            print(f\"Successfully initiated deletion for scan '{scan_id}'.\")\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 404:\n",
        "                print(f\"Scan '{scan_id}' not found, no need to delete.\")\n",
        "            else:\n",
        "                # Re-raise other HTTP errors during deletion.\n",
        "                raise\n",
        "    payload = {\n",
        "        \"displayName\": scan_id,\n",
        "        \"type\": \"DATA_DOCUMENTATION\",\n",
        "        \"data\": {\"resource\": dataset_resource},\n",
        "        \"dataDocumentationSpec\": {},\n",
        "        \"executionSpec\": {\"trigger\": {\"onDemand\": {}}}\n",
        "    }\n",
        "    print(f\"Attempting to create data scan '{scan_id}'...\")\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        operation_name = response.json().get(\"name\")\n",
        "        print(f\"Scan creation initiated. Operation: {operation_name}\")\n",
        "        return operation_name\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        # If a 409 conflict occurs, it means the resource exists, which is acceptable.\n",
        "        if e.response.status_code == 409:\n",
        "            print(f\"Scan '{scan_id}' already exists. Skipping creation and proceeding.\")\n",
        "            return None\n",
        "        else:\n",
        "            # For any other HTTP error, re-raise the exception.\n",
        "            raise\n",
        "\n",
        "def delete_data_scan(token, project_id, location, scan_id):\n",
        "    \"\"\"\n",
        "    Deletes a Dataplex data scan.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if deletion was successful or the scan didn't exist, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    # The API endpoint for deleting a specific data scan\n",
        "    url = f\"{BASE_URL_FOR_DATAPLEX_SCAN}/projects/{project_id}/locations/{location}/dataScans/{scan_id}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "    print(f\"Attempting to delete data scan '{scan_id}'...\")\n",
        "    try:\n",
        "        response = requests.delete(url, headers=headers)\n",
        "        # Raises an HTTPError for bad responses (4xx or 5xx)\n",
        "        response.raise_for_status()\n",
        "        print(f\"Successfully initiated deletion for scan '{scan_id}'.\")\n",
        "        # The API returns a long-running operation, you might need to wait for it.\n",
        "        return True\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        if e.response.status_code == 404:\n",
        "            print(f\"Scan '{scan_id}' not found. No action needed.\")\n",
        "            return True # Considered success as the scan is not present\n",
        "        else:\n",
        "            print(f\"An HTTP error occurred: {e}\")\n",
        "            print(f\"Response content: {e.response.text}\")\n",
        "            return False\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "def create_scan_jobs(dataset_type: str, scan_type) -> dict:\n",
        "    \"\"\"\n",
        "    Creates knowledge and documentation scans for a specified dataset.\n",
        "    The dataset can be the operational dataset or the star schema dataset, also known as\n",
        "    the data warehouse dataset.\n",
        "\n",
        "    Valid inputs for 'dataset_type' are 'OLTP' or 'OLAP'\n",
        "    Valid inputs for 'scan_type' are 'DATASET_DOCUMENTATION_SCAN' or 'TABLE_DOCUMENTATION_SCAN'\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate the 'dataset_type' parameter to ensure it's a valid choice.\n",
        "    valid_dataset_type = [\"OLTP\", \"OLAP\"]\n",
        "    if dataset_type not in valid_dataset_type:\n",
        "        raise ValueError(f\"Invalid dataset type '{dataset_type}' provided. Please use one of {valid_dataset_type}.\")\n",
        "\n",
        "    # Validate the 'scan_type' parameter to ensure it's a valid choice.\n",
        "    valid_scan_types = [\"DATASET_DOCUMENTATION_SCAN\", \"TABLE_DOCUMENTATION_SCAN\"]\n",
        "    if scan_type not in valid_scan_types:\n",
        "        raise ValueError(f\"Invalid scan type '{scan_type}' provided. Please use one of {valid_scan_types}.\")\n",
        "\n",
        "    print(f\"--- Tool: Starting Data Insights scans for the {dataset_type} dataset... ---\")\n",
        "\n",
        "    token = get_auth_token()\n",
        "\n",
        "    # Set the correct constants based on the dataset parameter.\n",
        "    if dataset_type == \"OLTP\":\n",
        "        dataset_resource = OLTP_DATASET_RESOURCE_URI\n",
        "        dataset_id = OLTP_DATASET_ID\n",
        "        knowledge_scan_id = sanitize_string_with_hyphens(dataset_id + \"-dataset-documentation-scan\")\n",
        "        metadata_dataset_id = OLTP_METADATA_DATASET_ID\n",
        "    else:  # This will be 'dwh' due to the validation above\n",
        "        dataset_resource = DWH_DATASET_RESOURCE_URI\n",
        "        dataset_id = DWH_DATASET_ID\n",
        "        knowledge_scan_id = sanitize_string_with_hyphens(dataset_id + \"-dataset-documentation-scan\")\n",
        "        metadata_dataset_id = DWH_METADATA_DATASET_ID\n",
        "\n",
        "    try:\n",
        "\n",
        "        scan_job = {}\n",
        "        scan_job_creation = {}\n",
        "        details= {}\n",
        "\n",
        "        if(scan_type == \"DATASET_DOCUMENTATION_SCAN\"):\n",
        "          # Create the knowledge scan and wait\n",
        "          delete_data_scan(token, PROJECT_ID, LOCATION, knowledge_scan_id)\n",
        "          op_name = create_dataset_documentation_scan_job(token, PROJECT_ID, LOCATION, dataset_resource, knowledge_scan_id)\n",
        "          patch_source_table_with_labels(token, dataset_id, \"DATA_KNOWLEDGE_ENGINE_SCAN\", knowledge_scan_id)\n",
        "          scan_job[knowledge_scan_id]=op_name\n",
        "          scan_job_creation[knowledge_scan_id]=False\n",
        "\n",
        "          details[knowledge_scan_id] = f\"Created the Data Insights scan called {knowledge_scan_id}.\"\n",
        "\n",
        "        else:\n",
        "          # Create the data scans for each table in the dataset\n",
        "          for table in get_dataset_tables(dataset_id):\n",
        "            data_scan_id = sanitize_string_with_hyphens(f\"{table.table_id}-table-documentation-scan\")\n",
        "            delete_data_scan(token, PROJECT_ID, LOCATION, data_scan_id)\n",
        "            op_name = create_table_data_documentation_scan_job(token, PROJECT_ID, LOCATION, dataset_resource, data_scan_id, table.table_id)\n",
        "            patch_source_table_with_labels(token, dataset_id, \"DATA_DOCUMENTATION_SCAN\", data_scan_id,  table.table_id)\n",
        "            scan_job[data_scan_id]=op_name\n",
        "            scan_job_creation[data_scan_id]=False\n",
        "            details[data_scan_id] = f\"Created the Data Insights scan for table {table} called {data_scan_id}.\"\n",
        "\n",
        "\n",
        "        # Pool constantly to see if all the scan jobs have been created\n",
        "        for _ in range(MAX_POLLS):\n",
        "          for data_scan_id, op_name in scan_job.items():\n",
        "            scan_job_creation[data_scan_id] = get_operation_success_status(token, op_name)\n",
        "\n",
        "            if all(scan_job_creation.values()):\n",
        "              return {\"status\": \"success\",\n",
        "                      \"Response\": f\"Successfully created the Data Insight scan(s) for the {dataset_type} dataset.\",\n",
        "                      \"Details\": details\n",
        "                      }\n",
        "\n",
        "            time.sleep(POLLING_INTERVAL_SECONDS)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred while generating the metadata: {e}\"\n",
        "        # Return a dictionary with an error status for ADK\n",
        "        return {\"status\": \"error\", \"error\": error_message}\n",
        "\n",
        "\n",
        "def run_scan_job(token, scan_id):\n",
        "    \"\"\"\n",
        "    Starts a job run for the data scan.\n",
        "    Returns the scan job name\n",
        "    \"\"\"\n",
        "\n",
        "    url = f\"{BASE_URL_FOR_DATAPLEX_SCAN}/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans/{scan_id}:run\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "    print(f\"Starting scan job for '{scan_id}'...\")\n",
        "    response = requests.post(url, headers=headers, data={})\n",
        "    response.raise_for_status()\n",
        "    job_name = response.json().get(\"job\", {}).get(\"name\")\n",
        "    print(f\"Scan job started. Job name: {job_name}\")\n",
        "    return job_name\n",
        "\n",
        "\n",
        "def run_scan_jobs(dataset_type: str, scan_type: str) -> dict:\n",
        "    \"\"\"\n",
        "    Starts the execution of the Data Insights scans (knowledge and documentation) for a specified\n",
        "    dataset. The dataset can be the operational dataset or the star schema dataset,\n",
        "    also know as the data warehouse dataset.\n",
        "\n",
        "    Valid inputs for 'dataset_type' are 'OLTP' or 'OLAP'\n",
        "    Valid inputs for 'scan_type' are 'DATASET_DOCUMENTATION_SCAN' or 'TABLE_DOCUMENTATION_SCAN'\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate the 'dataset_type' parameter to ensure it's a valid choice.\n",
        "    valid_dataset_type = [\"OLTP\", \"OLAP\"]\n",
        "    if dataset_type not in valid_dataset_type:\n",
        "        raise ValueError(f\"Invalid dataset type '{dataset_type}' provided. Please use one of {valid_dataset_type}.\")\n",
        "\n",
        "    # Validate the 'scan_type' parameter to ensure it's a valid choice.\n",
        "    valid_scan_types = [\"DATASET_DOCUMENTATION_SCAN\", \"TABLE_DOCUMENTATION_SCAN\"]\n",
        "    if scan_type not in valid_scan_types:\n",
        "        raise ValueError(f\"Invalid scan type '{scan_type}' provided. Please use one of {valid_scan_types}.\")\n",
        "\n",
        "    print(f\"--- Tool: Starting Data Insights scans for the {dataset_type} dataset... ---\")\n",
        "\n",
        "    token = get_auth_token()\n",
        "    details= {}\n",
        "\n",
        "    # 2. Set the correct constants based on the dataset parameter.\n",
        "    if dataset_type == \"OLTP\":\n",
        "        dataset_resource = OLTP_DATASET_RESOURCE_URI\n",
        "        dataset_id = OLTP_DATASET_ID\n",
        "        knowledge_scan_id = sanitize_string_with_hyphens(dataset_id + \"-dataset-documentation-scan\")\n",
        "        metadata_dataset_id = OLTP_METADATA_DATASET_ID\n",
        "    else:  # This will be 'dwh' due to the validation above\n",
        "        dataset_resource = DWH_DATASET_RESOURCE_URI\n",
        "        dataset_id = DWH_DATASET_ID\n",
        "        knowledge_scan_id = sanitize_string_with_hyphens(dataset_id + \"-dataset-documentation-scan\")\n",
        "        metadata_dataset_id = DWH_METADATA_DATASET_ID\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        if(scan_type == \"DATASET_DOCUMENTATION_SCAN\"):\n",
        "          # Run the knowledge scan\n",
        "          job_name = run_scan_job(token, knowledge_scan_id)\n",
        "          details[knowledge_scan_id]=f\"Started Data Insights job {knowledge_scan_id} for dataset {dataset_id}\"\n",
        "\n",
        "        else:\n",
        "          # Run the documentation data scans\n",
        "          for table in get_dataset_tables(f\"{dataset_id}\"):\n",
        "            data_scan_id = sanitize_string_with_hyphens(f\"{table.table_id}-table-documentation-scan\")\n",
        "            job_name = run_scan_job(token, data_scan_id)\n",
        "            details[data_scan_id]=f\"Started the Data Insights job {data_scan_id} for table {table}\"\n",
        "\n",
        "        return {\"status\": \"success\",\n",
        "                \"Response\": f\"Successfully scheduled the runs for all the Data Insights scans.\",\n",
        "                \"Details\": details\n",
        "                }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred during metadata generation: {e}\"\n",
        "        # Return a dictionary with an error status for ADK\n",
        "        return {\"status\": \"error\", \"error\": error_message}\n",
        "\n",
        "\n",
        "def get_latest_scan_jobs_run_status(dataset_type: str) -> dict:\n",
        "    \"\"\"\n",
        "    Checks the execution status of the Data Insights scans (knowledge and documentation)\n",
        "    for a specified dataset. The dataset can be the operational dataset or the star schema\n",
        "    dataset, also know as data warehouse dataset.\n",
        "\n",
        "    Valid inputs for 'dataset_type' are 'OLTP' or 'OLAP'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate the 'dataset' parameter to ensure it's a valid choice.\n",
        "    valid_datasets = [\"OLTP\", \"OLAP\"]\n",
        "    if dataset_type not in valid_datasets:\n",
        "        raise ValueError(f\"Invalid dataset '{dataset_type}' provided. Please use one of {valid_datasets}.\")\n",
        "\n",
        "    print(f\"--- Tool: Getting the Data Insight scans status for the {dataset_type} dataset... ---\")\n",
        "\n",
        "    token = get_auth_token()\n",
        "\n",
        "    # Set the correct constants based on the dataset parameter.\n",
        "    if dataset_type == \"OLTP\":\n",
        "        dataset_resource = OLTP_DATASET_RESOURCE_URI\n",
        "        dataset_id = OLTP_DATASET_ID\n",
        "        knowledge_scan_id = sanitize_string_with_hyphens(dataset_id + \"-dataset-documentation-scan\")\n",
        "        metadata_dataset_id = OLTP_METADATA_DATASET_ID\n",
        "    else:  # This will be 'dwh' due to the validation above\n",
        "        dataset_resource = DWH_DATASET_RESOURCE_URI\n",
        "        dataset_id = DWH_DATASET_ID\n",
        "        knowledge_scan_id = sanitize_string_with_hyphens(dataset_id + \"-dataset-documentation-scan\")\n",
        "        metadata_dataset_id = DWH_METADATA_DATASET_ID\n",
        "\n",
        "    try:\n",
        "        scan_job_details = {}\n",
        "\n",
        "        # Run the knowledge scan\n",
        "        scan_job_details[knowledge_scan_id] = get_latest_dataplex_scan_job_details(token, knowledge_scan_id)\n",
        "\n",
        "        # Run the documentation data scans\n",
        "        for table in get_dataset_tables(f\"{dataset_id}\"):\n",
        "          data_scan_id = sanitize_string_with_hyphens(f\"{table.table_id}-table-documentation-scan\")\n",
        "          scan_job_details[data_scan_id] = get_latest_dataplex_scan_job_details(token, data_scan_id)\n",
        "\n",
        "        print(scan_job_details)\n",
        "\n",
        "        running_jobs = sum(details['state'] == 'RUNNING' for details in scan_job_details.values() if details)\n",
        "        succeeded_jobs = sum(details['state'] == 'SUCCEEDED' for details in scan_job_details.values() if details)\n",
        "        failed_jobs = sum(details['state'] == 'FAILED' for details in scan_job_details.values() if details)\n",
        "        pending_jobs = sum(details['state'] == 'PENDING' for details in scan_job_details.values() if details)\n",
        "\n",
        "\n",
        "        overall_status = f\"{running_jobs} running jobs, {succeeded_jobs} jobs succeeded, {failed_jobs} jobs failed, and {pending_jobs} jobs are pending out of {len(scan_job_details)} total jobs\"\n",
        "\n",
        "        if failed_jobs > 0:\n",
        "            return {\"status\": \"error\", \"final\": \"yes\", \"error\": f\"{overall_status}. Please run the Data Insights job creation and execution steps again.\", \"Details\": scan_job_details, \"Overview\": scan_job_details}\n",
        "        elif pending_jobs+running_jobs > 0:\n",
        "            return {\"status\": \"pending\", \"final\": \"no\", \"Response\": f\"{overall_status}. Please check again in a few moments...\", \"Details\":scan_job_details, \"Overview\": scan_job_details}\n",
        "        else:\n",
        "            return {\"status\": \"success\", \"final\": \"yes\", \"Response\": f\"{overall_status}.\", \"Details\":scan_job_details, \"Overview\": scan_job_details}\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred during metadata generation: {e}\"\n",
        "        return {\"status\": \"error\", \"error\": error_message}\n",
        "\n",
        "def get_latest_dataplex_scan_job_details(token: str, scan_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    Gets the latest run job state and elapsed time for a certain scan id.\n",
        "    \"\"\"\n",
        "    url = f\"{BASE_URL_FOR_DATAPLEX_SCAN}/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans/{scan_id}/jobs\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        jobs = response.json().get(\"dataScanJobs\", [])\n",
        "        if jobs:\n",
        "            sorted_jobs = sorted(jobs, key=lambda x: x.get('createTime', ''), reverse=True)\n",
        "            latest_job = sorted_jobs[0]\n",
        "\n",
        "            state = latest_job.get(\"state\")\n",
        "            start_time_str = latest_job.get(\"startTime\")\n",
        "            end_time_str = latest_job.get(\"endTime\")\n",
        "\n",
        "            elapsed_time_str = \"Not started\"\n",
        "            if start_time_str:\n",
        "                try:\n",
        "\n",
        "                    def parse_timestamp(ts_str):\n",
        "                        # truncating microseconds to 6 digits\n",
        "                        if '.' in ts_str:\n",
        "                            main_part, frac_part = ts_str.split('.', 1)\n",
        "                            frac_part = frac_part.replace('Z', '')[:6]\n",
        "                            ts_str = f\"{main_part}.{frac_part}\"\n",
        "                            return datetime.datetime.strptime(ts_str, \"%Y-%m-%dT%H:%M:%S.%f\").replace(tzinfo=datetime.timezone.utc)\n",
        "                        else:\n",
        "                            ts_str = ts_str.replace('Z', '')\n",
        "                            return datetime.datetime.strptime(ts_str, \"%Y-%m-%dT%H:%M:%S\").replace(tzinfo=datetime.timezone.utc)\n",
        "\n",
        "                    start_time = parse_timestamp(start_time_str)\n",
        "\n",
        "                    end_time = datetime.datetime.now(datetime.timezone.utc)\n",
        "                    if end_time_str:\n",
        "                        end_time = parse_timestamp(end_time_str)\n",
        "\n",
        "                    elapsed_time = end_time - start_time\n",
        "                    total_seconds = int(elapsed_time.total_seconds())\n",
        "                    hours, remainder = divmod(total_seconds, 3600)\n",
        "                    minutes, seconds = divmod(remainder, 60)\n",
        "                    elapsed_time_str = f\"{hours}h {minutes}m {seconds}s\"\n",
        "                except Exception as e:\n",
        "                    elapsed_time_str = f\"Could not calculate elapsed time: {e}\"\n",
        "\n",
        "            return {\"state\": state, \"elapsed_time\": elapsed_time_str}\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"An HTTP error occurred: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "OvbFbw7WClnh"
      },
      "id": "OvbFbw7WClnh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_dataset(description_df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Converts the dataset description DataFrame into a human-readable string.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the dataset's description.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extracts the first description from the DataFrame\n",
        "        description = description_df[\"dataset_description\"].iloc[0]\n",
        "        return f\"Dataset Overview:\\n{description}\"\n",
        "    except (IndexError, KeyError):\n",
        "        return \"No dataset description was found.\"\n",
        "\n",
        "def describe_relationships(relationships_df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Converts the relationships DataFrame into human-readable sentences.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string describes a table relationship.\n",
        "    \"\"\"\n",
        "    descriptions = []\n",
        "    for _, row in relationships_df.iterrows():\n",
        "        # Constructs a sentence describing the join between two tables\n",
        "        sentence = (\n",
        "            f\"Table '{row['table_1']}' connects to table '{row['table_2']}' by joining \"\n",
        "            f\"'{row['table_1']}.{row['table_1_column']}' with '{row['table_2']}.{row['table_2_column']}'.\"\n",
        "        )\n",
        "        descriptions.append(sentence)\n",
        "    return descriptions\n",
        "\n",
        "def describe_tables(tables_df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Converts the tables DataFrame into human-readable descriptions.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string is a description of a table.\n",
        "    \"\"\"\n",
        "    descriptions = []\n",
        "    for _, row in tables_df.iterrows():\n",
        "        # Formats a description for each table\n",
        "        description = f\"Table '{row['name']}': {row['description']}\"\n",
        "        descriptions.append(description)\n",
        "    return descriptions\n",
        "\n",
        "def describe_columns(columns_df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Converts the columns DataFrame into a structured, human-readable text block.\n",
        "\n",
        "    Returns:\n",
        "        A single string that describes the columns for each table.\n",
        "    \"\"\"\n",
        "    full_description = \"Column Details for Each Table:\\n\"\n",
        "    # Group the DataFrame by table name to process each table's columns together\n",
        "    for table_name, group in columns_df.groupby(\"table_name\"):\n",
        "        full_description += f\"\\n--- Table: {table_name} ---\\n\"\n",
        "        for _, row in group.iterrows():\n",
        "            # Add a formatted line for each column's name and description\n",
        "            full_description += f\"- {row['column_name']}: {row['column_description']}\\n\"\n",
        "    return full_description\n",
        "\n",
        "\n",
        "def get_scan_results(token,scan_id):\n",
        "    \"\"\"\n",
        "    Retrieves the results of the data scan.\n",
        "    \"\"\"\n",
        "    print(\"Fetching scan results...\")\n",
        "\n",
        "    url = f\"{BASE_URL_FOR_DATAPLEX_SCAN}/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans/{scan_id}?view=FULL\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    print(\"Successfully fetched scan results.\")\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def persist_dataplex_scan_output_to_bq_tables(dataset_type: str) -> dict:\n",
        "    \"\"\"\n",
        "    Saves to BigQuery the Data Insights scans (knowledge and documentation)\n",
        "    for a specified dataset. The dataset can be the operational dataset or the star schema\n",
        "    dataset, also know as data warehouse dataset.\n",
        "\n",
        "    Valid inputs for 'dataset_type' are 'OLTP' or 'OLAP'.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Validate the 'dataset' parameter to ensure it's a valid choice.\n",
        "    valid_dataset_types = [\"OLTP\", \"OLAP\"]\n",
        "    if dataset_type not in valid_dataset_types:\n",
        "        raise ValueError(f\"Invalid dataset '{dataset_type}' provided. Please use one of {valid_dataset_types}.\")\n",
        "\n",
        "    print(f\"--- Tool: Starting saving the Data Insights scans for the {dataset_type} dataset... ---\")\n",
        "\n",
        "    token = get_auth_token()\n",
        "\n",
        "    # 2. Set the correct constants based on the dataset parameter.\n",
        "    if dataset_type == \"OLTP\":\n",
        "        dataset_resource = OLTP_DATASET_RESOURCE_URI\n",
        "        dataset_id = OLTP_DATASET_ID\n",
        "        knowledge_scan_id = sanitize_string_with_hyphens(dataset_id + \"-dataset-documentation-scan\")\n",
        "        metadata_dataset_id = OLTP_METADATA_DATASET_ID\n",
        "    else:  # This will be 'dwh' due to the validation above\n",
        "        dataset_resource = DWH_DATASET_RESOURCE_URI\n",
        "        dataset_id = DWH_DATASET_ID\n",
        "        knowledge_scan_id = sanitize_string_with_hyphens(dataset_id + \"-dataset-documentation-scan\")\n",
        "        metadata_dataset_id = DWH_METADATA_DATASET_ID\n",
        "\n",
        "\n",
        "    table_scan_suffix = \"table-documentation-scan\"\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        relationships = {}\n",
        "        details = {}\n",
        "\n",
        "        # Fetch the knowledge scan and display the results\n",
        "        results = get_scan_results(token, knowledge_scan_id)\n",
        "\n",
        "\n",
        "        # Part 1: Dataset description\n",
        "        datasetDescription = results.get(\"dataDocumentationResult\", {}).get(\"datasetResult\", {}).get(\"overview\")\n",
        "        details[\"Description\"] = datasetDescription\n",
        "\n",
        "        data = {\n",
        "                \"dataset_description\": datasetDescription\n",
        "        }\n",
        "\n",
        "        if datasetDescription is not None:\n",
        "            truncate_bigquery_table(f\"{PROJECT_ID}.{metadata_dataset_id}.dataset_description\")\n",
        "            write_dict_to_bigquery(f\"{PROJECT_ID}.{metadata_dataset_id}.dataset_description\", data)\n",
        "            update_bigquery_metadata(PROJECT_ID, dataset_id, datasetDescription)\n",
        "            #print(\"PART 1 - Persisted dataset description\")\n",
        "\n",
        "\n",
        "        # Part 2: Table relationships\n",
        "        schemaRelationships = results.get(\"dataDocumentationResult\", {}).get(\"datasetResult\", {}).get(\"schemaRelationships\")\n",
        "        details[\"Relationships\"] = schemaRelationships\n",
        "        if schemaRelationships is not None:\n",
        "            truncate_bigquery_table(f\"{PROJECT_ID}.{metadata_dataset_id}.dataset_table_relationships\")\n",
        "\n",
        "        # Use a 'for' loop to iterate over each element in the list\n",
        "        for i, relationship in enumerate(schemaRelationships):\n",
        "            # Now 'relationship' is one of the dictionaries from the list\n",
        "\n",
        "            # Safely access the data inside each dictionary\n",
        "            join_type = relationship.get(\"type\", \"Unknown Type\").replace(\"SCHEMA_JOIN\", \"JOIN\")\n",
        "\n",
        "            # Reset\n",
        "            left_table = \"N/A\"\n",
        "            left_column = \"N/A\"\n",
        "            right_table = \"N/A\"\n",
        "            right_column = \"N/A\"\n",
        "\n",
        "            # Access elements of interest\n",
        "            left_table_fqn = relationship.get(\"leftSchemaPaths\", []).get(\"tableFqn\", {})\n",
        "            left_table_resource_uri_parts = left_table_fqn.split(\"/\")\n",
        "            left_table=left_table_resource_uri_parts[8]\n",
        "            left_table_column = relationship.get(\"leftSchemaPaths\", []).get(\"paths\", {})[0]\n",
        "\n",
        "            right_table_fqn = relationship.get(\"rightSchemaPaths\", []).get(\"tableFqn\", {})\n",
        "            right_table_resource_uri_parts = right_table_fqn.split(\"/\")\n",
        "            right_table=  right_table_resource_uri_parts[8]\n",
        "            right_table_column = relationship.get(\"rightSchemaPaths\", []).get(\"paths\", {})[0]\n",
        "\n",
        "            row_data = {\n",
        "                \"table_1\": left_table,\n",
        "                \"table_1_column\": left_table_column,\n",
        "                \"table_2\": right_table,\n",
        "                \"table_2_column\": right_table_column,\n",
        "                \"join_type\": join_type\n",
        "            }\n",
        "\n",
        "            write_dict_to_bigquery(f\"{PROJECT_ID}.{metadata_dataset_id}.dataset_table_relationships\", row_data)\n",
        "\n",
        "\n",
        "        #print(\"PART 2 - Persisted dataset table relationships\")\n",
        "\n",
        "\n",
        "        # Part 3: Tables\n",
        "        tableResults = results.get(\"dataDocumentationResult\", {}).get(\"datasetResult\", {}).get(\"tableResults\", {})\n",
        "        details[\"tableResults\"] = tableResults\n",
        "        if tableResults is not None:\n",
        "            truncate_bigquery_table(f\"{PROJECT_ID}.{metadata_dataset_id}.table_descriptions\")\n",
        "            truncate_bigquery_table(f\"{PROJECT_ID}.{metadata_dataset_id}.table_column_descriptions\")\n",
        "\n",
        "        for i, tableResult in enumerate(tableResults):\n",
        "          table_nm=\"\"\n",
        "          table_description=\"\"\n",
        "\n",
        "          table_fqn=tableResult.get(\"name\", \"\")\n",
        "          table_resource_uri_parts = table_fqn.split(\"/\")\n",
        "          table_nm=table_resource_uri_parts[8]\n",
        "          table_description=tableResult.get(\"overview\", \"\")\n",
        "\n",
        "          row_data = {\n",
        "                \"name\": table_nm,\n",
        "                \"description\": table_description,\n",
        "            }\n",
        "\n",
        "          # Persist table description\n",
        "          write_dict_to_bigquery(f\"{PROJECT_ID}.{metadata_dataset_id}.table_descriptions\", row_data)\n",
        "          #print(\"PART 3 - Persisted table metadata\")\n",
        "\n",
        "          # Part 4: Persist table columns and descriptions\n",
        "          tableColumnResults = tableResult.get(\"schema\", {}).get(\"fields\", [])\n",
        "          details[\"tableColumnResults\"] = tableColumnResults\n",
        "\n",
        "          for i, tableColumnResult in enumerate(tableColumnResults):\n",
        "\n",
        "            table_column_nm=\"None\"\n",
        "            table_column_description=\"None\"\n",
        "\n",
        "            table_column_nm=tableColumnResult.get(\"name\")\n",
        "            table_column_description=tableColumnResult.get(\"description\")\n",
        "\n",
        "            row_data = {\n",
        "                  \"table_name\": table_nm,\n",
        "                  \"column_name\": table_column_nm,\n",
        "                  \"column_description\": table_column_description,\n",
        "              }\n",
        "\n",
        "            # Persist table column descriptions\n",
        "            write_dict_to_bigquery(f\"{PROJECT_ID}.{metadata_dataset_id}.table_column_descriptions\", row_data)\n",
        "\n",
        "          #print(\"PART 4 - Persisted table column metadata\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred during metadata generation: {e}\"\n",
        "        # Return a dictionary with an error status for ADK\n",
        "        return {\"status\": \"error\", \"error\": error_message}\n",
        "\n",
        "    return {\"status\": \"success\",\n",
        "            \"Response\": f\"Successfully saved the Data scans for the {dataset_type} dataset.\",\n",
        "            \"Details\": details\n",
        "            }\n"
      ],
      "metadata": {
        "id": "-dgFYimNZkZL"
      },
      "id": "-dgFYimNZkZL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Reporting utils"
      ],
      "metadata": {
        "id": "wB8gPusaj295"
      },
      "id": "wB8gPusaj295"
    },
    {
      "cell_type": "code",
      "source": [
        "PIPELINE_ID = f\"projects/{PROJECT_ID}/locations/{LOCATION}/repositories/{DATAFORM_REPO}/workspaces/{DATAFORM_WORKSPACE}\"\n",
        "\n",
        "# Instruction files\n",
        "REPORTING_NAMING_CONVENTIONS_FILE = \"03-reporting-naming-conventions.md\"\n",
        "REPORTING_NAMING_CONVENTIONS_CONTENT = \"\"\"\n",
        "Naming conventions for the reporting dataset:\n",
        "\n",
        "* Tables:\n",
        "\n",
        "    - Report tables: rpt_[report_name]\n",
        "\n",
        "Always use ${ref()} to ensure dependency between nodes in dataform. ex: ${ref(\"referenced_sqlx_file\")}\n",
        "Always create a date dimension table that contains the year, month and date and link it properly to all the facts.\n",
        "Always the string 'reports' and the source table names must appear as tags in the config block of the .sqlx definition files for each report.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "3DMy7lvR_c1F"
      },
      "id": "3DMy7lvR_c1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_stage_for_report_generation():\n",
        "    \"\"\"\n",
        "    1. Persists the Data Insights scan results for the star schema dataset to instructions files for\n",
        "    the Data Engineering Agent.\n",
        "    2. Generates the naming conventions instructions file and saves it to the Dataform workspace\n",
        "    3. Generates the agent grounding instructions file (metadata from the Dataplex scans) and saves it to the Dataform workspace\n",
        "    \"\"\"\n",
        "\n",
        "    agent_grounding_content = \"\"\n",
        "\n",
        "    token = get_auth_token()\n",
        "\n",
        "\n",
        "    # Persist the latest Dataplex scan data to a separate dataset for use for agentic grounding\n",
        "    persist_dataplex_scan_output_to_bq_tables(\"OLAP\")\n",
        "    print(\"1. Persisted Dataplex scan information to BQ dataset\")\n",
        "\n",
        "    # Generate agent grounding file content\n",
        "    dwh_description_df = read_bigquery_table(PROJECT_ID,DWH_METADATA_DATASET_ID, \"dataset_description\")\n",
        "    dwh_relationships_df = read_bigquery_table(PROJECT_ID,DWH_METADATA_DATASET_ID, \"dataset_table_relationships\")\n",
        "    dwh_tables_df = read_bigquery_table(PROJECT_ID,DWH_METADATA_DATASET_ID, \"table_descriptions\")\n",
        "    dwh_columns_df = read_bigquery_table(PROJECT_ID,DWH_METADATA_DATASET_ID, \"table_column_descriptions\")\n",
        "\n",
        "    agent_grounding_instruction_file = \"04-agent-grounding.md\"\n",
        "    agent_grounding_content = describe_dataset(dwh_description_df)\n",
        "    agent_grounding_content += \"\\n\"\n",
        "    agent_grounding_content += \"------------------------------------\"\n",
        "    agent_grounding_content += \"\\n\"\n",
        "    agent_grounding_content += f\"Details for each table in the star schema dataset {DWH_METADATA_DATASET_ID}:\"\n",
        "    agent_grounding_content += \"\\n\"\n",
        "    agent_grounding_content += \"\\n\".join(describe_tables(dwh_tables_df))\n",
        "    agent_grounding_content += \"\\n\"\n",
        "    agent_grounding_content += \"------------------------------------\"\n",
        "    agent_grounding_content += \"\\n\"\n",
        "    agent_grounding_content += f\"Details on the relationships between tables in the star schema dataset {DWH_METADATA_DATASET_ID}:\"\n",
        "    agent_grounding_content += \"\\n\"\n",
        "    agent_grounding_content += \"\\n\".join(describe_relationships(dwh_relationships_df))\n",
        "    agent_grounding_content += \"\\n\"\n",
        "    agent_grounding_content += \"------------------------------------\"\n",
        "\n",
        "    agent_grounding_content += f\"Details on each column of the tables in the star schema dataset {DWH_METADATA_DATASET_ID}:\"\n",
        "    agent_grounding_content += describe_columns(dwh_columns_df)\n",
        "    agent_grounding_content += \"------------------------------------\"\n",
        "\n",
        "    print(\"2. Gathered agent grounding content\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        result = create_dataform_instructions_file(\n",
        "            token,\n",
        "            PROJECT_ID,\n",
        "            LOCATION,\n",
        "            DATAFORM_REPO,\n",
        "            DATAFORM_WORKSPACE,\n",
        "            filename=REPORTING_NAMING_CONVENTIONS_FILE,\n",
        "            content=REPORTING_NAMING_CONVENTIONS_CONTENT\n",
        "        )\n",
        "\n",
        "        print(f\"3. Successfully created the instruction file: {REPORTING_NAMING_CONVENTIONS_FILE}\")\n",
        "\n",
        "\n",
        "\n",
        "        result = create_dataform_instructions_file(\n",
        "              token,\n",
        "              PROJECT_ID,\n",
        "              LOCATION,\n",
        "              DATAFORM_REPO,\n",
        "              DATAFORM_WORKSPACE,\n",
        "              filename=agent_grounding_instruction_file,\n",
        "              content=agent_grounding_content\n",
        "          )\n",
        "\n",
        "        print(f\"4. Successfully created the instruction file: {agent_grounding_instruction_file}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred while generating the analytical reports: {e}\"\n",
        "        # Return a dictionary with an error status for ADK\n",
        "        return {\"status\": \"error\", \"error\": error_message}\n",
        "\n",
        "    return {\"status\": \"success\",\n",
        "            \"Response\": \"Successfully completed all the pre-work.\",\n",
        "            }"
      ],
      "metadata": {
        "id": "1KXx9tQ8CBDS"
      },
      "id": "1KXx9tQ8CBDS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate Dataplex Scans for agentic grounding for the Data Warehouse tables"
      ],
      "metadata": {
        "id": "bLdI-0_Sb1jq"
      },
      "id": "bLdI-0_Sb1jq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Create & run table documentation scans to completion"
      ],
      "metadata": {
        "id": "rXd-m2ldg2ZL"
      },
      "id": "rXd-m2ldg2ZL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.1. Create table documentation scan for all tables in the data warehouse dataset"
      ],
      "metadata": {
        "id": "9fEShjUDhItv"
      },
      "id": "9fEShjUDhItv"
    },
    {
      "cell_type": "code",
      "source": [
        "create_scan_jobs(\"OLAP\", \"TABLE_DOCUMENTATION_SCAN\")"
      ],
      "metadata": {
        "id": "RNvob2Ygb4dM"
      },
      "id": "RNvob2Ygb4dM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.2. Execute the table documentation scan jobs for all the tables in the Data Warehouse dataset"
      ],
      "metadata": {
        "id": "AEHWv7vVg-M_"
      },
      "id": "AEHWv7vVg-M_"
    },
    {
      "cell_type": "code",
      "source": [
        "run_scan_jobs(\"OLAP\", \"TABLE_DOCUMENTATION_SCAN\")"
      ],
      "metadata": {
        "id": "Q8Z47_lBcyBO"
      },
      "id": "Q8Z47_lBcyBO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.3. Wait for completion of all the table documentation scan jobs\n",
        "\n",
        "\n",
        "Run this function over and over again until all jobs show as \"SUCCEEDED\" before proceeding to the next cell"
      ],
      "metadata": {
        "id": "Jph3fNWzhcRT"
      },
      "id": "Jph3fNWzhcRT"
    },
    {
      "cell_type": "code",
      "source": [
        "get_latest_scan_jobs_run_status(\"OLAP\")"
      ],
      "metadata": {
        "id": "Bs-EeGlhfUXi"
      },
      "id": "Bs-EeGlhfUXi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Create and run dataset documentation scans (formerly called knowledge engine scans)"
      ],
      "metadata": {
        "id": "T7DNp1a1hw4-"
      },
      "id": "T7DNp1a1hw4-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1. Create the scan"
      ],
      "metadata": {
        "id": "jfYJ2Lsnh5zi"
      },
      "id": "jfYJ2Lsnh5zi"
    },
    {
      "cell_type": "code",
      "source": [
        "create_scan_jobs(\"OLAP\", \"DATASET_DOCUMENTATION_SCAN\")"
      ],
      "metadata": {
        "id": "z_hpiedfcZeg"
      },
      "id": "z_hpiedfcZeg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2. Run the scan"
      ],
      "metadata": {
        "id": "a7yElvNXiCTQ"
      },
      "id": "a7yElvNXiCTQ"
    },
    {
      "cell_type": "code",
      "source": [
        "run_scan_jobs(\"OLAP\", \"DATASET_DOCUMENTATION_SCAN\")"
      ],
      "metadata": {
        "id": "d2pCq0Ede7Oc"
      },
      "id": "d2pCq0Ede7Oc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.3. Wait for its completion\n",
        "\n",
        "Run this function over and over again until all jobs show as \"SUCCEEDED\" before proceeding to the next cell"
      ],
      "metadata": {
        "id": "Nbttoa01iF4j"
      },
      "id": "Nbttoa01iF4j"
    },
    {
      "cell_type": "code",
      "source": [
        "get_latest_scan_jobs_run_status(\"OLAP\")"
      ],
      "metadata": {
        "id": "JDBiFc_qgadg"
      },
      "id": "JDBiFc_qgadg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Complete the pre-work needed for the Data Engineering Agent to generate the reporting mart"
      ],
      "metadata": {
        "id": "3YyUEqrzZlgr"
      },
      "id": "3YyUEqrzZlgr"
    },
    {
      "cell_type": "code",
      "source": [
        "set_stage_for_report_generation()"
      ],
      "metadata": {
        "id": "RbByVaBQVo1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764128673526,
          "user_tz": 360,
          "elapsed": 211006,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "975d0f4d-7025-4384-cb40-a70b2cb2c5a5"
      },
      "id": "RbByVaBQVo1y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: Starting saving the Data Insights scans for the OLAP dataset... ---\n",
            "Fetching scan results...\n",
            "Successfully fetched scan results.\n",
            "Executing query: TRUNCATE TABLE `data-insights-quickstart.rscw_dwh_metadata_ds.dataset_description`\n",
            "Successfully updated description for dataset: data-insights-quickstart.rscw_dwh_ds\n",
            "Executing query: TRUNCATE TABLE `data-insights-quickstart.rscw_dwh_metadata_ds.dataset_table_relationships`\n",
            "Executing query: TRUNCATE TABLE `data-insights-quickstart.rscw_dwh_metadata_ds.table_descriptions`\n",
            "Executing query: TRUNCATE TABLE `data-insights-quickstart.rscw_dwh_metadata_ds.table_column_descriptions`\n",
            "1. Persisted Dataplex scan information to BQ dataset\n",
            "Reading all rows from table: data-insights-quickstart.rscw_dwh_metadata_ds.dataset_description...\n",
            "Successfully read 1 rows.\n",
            "Reading all rows from table: data-insights-quickstart.rscw_dwh_metadata_ds.dataset_table_relationships...\n",
            "Successfully read 16 rows.\n",
            "Reading all rows from table: data-insights-quickstart.rscw_dwh_metadata_ds.table_descriptions...\n",
            "Successfully read 10 rows.\n",
            "Reading all rows from table: data-insights-quickstart.rscw_dwh_metadata_ds.table_column_descriptions...\n",
            "Successfully read 83 rows.\n",
            "2. Gathered agent grounding content\n",
            "3. Successfully created the instruction file: 03-reporting-naming-conventions.md\n",
            "4. Successfully created the instruction file: 04-agent-grounding.md\n",
            "5. Successfully created the instruction file: 05-timesfm-in-bq.md\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'success', 'Response': 'Successfully completed all the pre-work.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This concludes the pre-work needed for reporting mart code generation. Proceed to the user manual for further instructions."
      ],
      "metadata": {
        "id": "KBm2YDNzPbm9"
      },
      "id": "KBm2YDNzPbm9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Module_04_Data_Engineering_Agent_For_Reporting",
      "collapsed_sections": [
        "bLdI-0_Sb1jq",
        "9fEShjUDhItv",
        "AEHWv7vVg-M_",
        "Jph3fNWzhcRT",
        "jfYJ2Lsnh5zi",
        "a7yElvNXiCTQ",
        "Nbttoa01iF4j"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}