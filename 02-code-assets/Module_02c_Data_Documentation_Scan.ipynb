{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "zphoba6nu9Qk"
      },
      "id": "zphoba6nu9Qk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 02c: Data Documentation Scan\n",
        "\n",
        "In the previous notebook, you ran Data Documentation Scans against individual tables. In this notebook, you will run a Data Documentation Scan against the dataset itself - rscw_oltp_stg_ds.\n",
        "\n",
        "**Motivation:** <br>\n",
        "\n",
        "Data Documentation Scan run against the dataset generates -\n",
        "1. Dataset description\n",
        "2. Infers the relationship between tables.\n",
        "3. Golden queries spanning tables in the dataset (versus just a single table)\n",
        "\n",
        "<br>\n",
        "\n",
        "Note: This feature was previously called Knowledge Engine Scan.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Prerequisites:** <br>\n",
        "Complete the prerequisites/dependencies detailed in the user module associated with this notebook.\n"
      ],
      "metadata": {
        "id": "xR5mqOl7OUXO"
      },
      "id": "xR5mqOl7OUXO"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json, time, logging\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import GoogleAPIError\n",
        "from google.api_core.exceptions import NotFound\n",
        "from google.api_core import exceptions\n",
        "from urllib.parse import urlencode # Import urlencode\n",
        "\n",
        "\n",
        "\n",
        "def get_access_token():\n",
        "    \"\"\"\n",
        "    Generates an access token using Application Default Credentials or a service account key file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Authenticate using Application Default Credentials (ADC)\n",
        "        # This will automatically find credentials set via `gcloud auth application-default login`\n",
        "        # or from the environment if running on GCP.\n",
        "        credentials, project = google.auth.default(scopes=SCOPES)\n",
        "\n",
        "        # Refresh the credentials to ensure an up-to-date access token\n",
        "        request = google.auth.transport.requests.Request()\n",
        "        credentials.refresh(request)\n",
        "        return credentials.token\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating access token: {e}\")\n",
        "        return None\n",
        "\n",
        "def patch_source_table_with_labels(access_token, dataset_id,scan_type, scan_id,  source_table_nm):\n",
        "    \"\"\"\n",
        "    Patches the source BigQuery table with labels to correlate with the data scans\n",
        "\n",
        "    Args:\n",
        "        access_token (str): The Google Cloud access token\n",
        "        dataset_id (str): The dataset id\n",
        "        scan_type (str):\n",
        "        scan_id (str): The scan id\n",
        "        source_table_nm (str): The source table name\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "            msg=\"Access token is missing. Cannot proceed with API call.\"\n",
        "            return msg\n",
        "\n",
        "    API_ENDPOINT=f\"https://bigquery.googleapis.com/bigquery/v2/projects/{PROJECT_ID}/datasets/{dataset_id}/tables/{source_table_nm}?\"\n",
        "\n",
        "    patch_request_body= generate_patch_label_request_body(scan_type,scan_id)\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "\n",
        "    # Apply the patch\n",
        "    try:\n",
        "        response = requests.patch(API_ENDPOINT, headers=headers, json=patch_request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "\n",
        "\n",
        "def generate_scan_request_body(dataset_id,scan_id, scan_type,\n",
        "                                            source_table_nm, profile_results_table_nm):\n",
        "    \"\"\"\n",
        "    Generates the scan request body\n",
        "\n",
        "    Args:\n",
        "        scan_id (str): scan id\n",
        "        scan_type (str): Type of scan (DATA_PROFLE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN)\n",
        "        source_table_nm (str): Source table name\n",
        "        profile_results_table_nm (str): The profile results table name\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        string: JSON with the request body for the data scan API call\n",
        "    \"\"\"\n",
        "    scan_request_body={}\n",
        "    if(scan_type == \"DATA_PROFILE_SCAN\"):\n",
        "\n",
        "        scan_request_body={\n",
        "            \"displayName\": f\"{scan_id}\",\n",
        "            \"data\": {\n",
        "                \"resource\": f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{dataset_id}/tables/{source_table_nm}\"\n",
        "            },\n",
        "            \"dataProfileSpec\": {\n",
        "                \"postScanActions\":\n",
        "                {\n",
        "                    \"bigqueryExport\":\n",
        "                    {\n",
        "                        \"resultsTable\": f\"projects/{PROJECT_ID}/datasets/{SCAN_RESULTS_BQ_DATASET_ID}/tables/{profile_results_table_nm}\"\n",
        "                    }\n",
        "                    }\n",
        "\n",
        "            },\n",
        "            \"executionSpec\": {\n",
        "                \"trigger\": {\n",
        "                    \"onDemand\": {} # Run on demand for this example\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    elif(scan_type == \"DATA_DOCUMENTATION_SCAN\"):\n",
        "        scan_request_body={\n",
        "        \"displayName\": f\"{scan_id}\",\n",
        "        \"type\": \"DATA_DOCUMENTATION\",\n",
        "        \"dataDocumentationSpec\": {},\n",
        "        \"data\": {\n",
        "            \"resource\": f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{dataset_id}/tables/{source_table_nm}\"\n",
        "        },\n",
        "        \"executionSpec\": {\n",
        "            \"trigger\": {\n",
        "                \"onDemand\": {} # Run on demand for this example\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    elif(scan_type == \"DATA_KNOWLEDGE_ENGINE_SCAN\"):\n",
        "        scan_request_body={\n",
        "        \"displayName\": f\"{scan_id}\",\n",
        "        \"type\": \"DATA_DOCUMENTATION\",\n",
        "        \"dataDocumentationSpec\": {},\n",
        "        \"dataDocumentationResult\": {},\n",
        "        \"data\": {\n",
        "            \"resource\": f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{dataset_id}\"\n",
        "        },\n",
        "        \"executionSpec\": {\n",
        "            \"trigger\": {\n",
        "                \"onDemand\": {} # Run on demand for this example\n",
        "            }\n",
        "        },\n",
        "\n",
        "    }\n",
        "\n",
        "    return scan_request_body\n",
        "\n",
        "def generate_patch_label_request_body(scan_type, scan_id):\n",
        "    \"\"\"\n",
        "        Returns the patch labels json that needs to be attached to the source table to tie programmatic scans to the UI\n",
        "\n",
        "        Args:\n",
        "            scan_type (str): Type of scan (DATA_PROFLE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN)\n",
        "            scan_id (str): Scan id\n",
        "            operation_type (str): Type of operation (CREATE_SCAN/RUN_SCAN)\n",
        "\n",
        "        Returns:\n",
        "            string: json with the patch labels\n",
        "        \"\"\"\n",
        "    label_json=\"\"\n",
        "    scan_stub = \"\"\n",
        "    if scan_type == \"DATA_PROFILE_SCAN\":\n",
        "        scan_stub=\"dp\"\n",
        "    elif scan_type == \"DATA_DOCUMENTATION_SCAN\":\n",
        "        scan_stub=\"data-documentation\"\n",
        "    elif scan_type == \"DATA_KNOWLEDGE_ENGINE_SCAN\":\n",
        "        scan_stub=\"data-documentation\"\n",
        "\n",
        "\n",
        "    label_json = {\n",
        "        \"labels\": {f\"dataplex-{scan_stub}-published-scan\":f\"{scan_id}\",\n",
        "                 f\"dataplex-{scan_stub}-published-project\":f\"{PROJECT_ID}\",\n",
        "                 f\"dataplex-{scan_stub}-published-location\":f\"{LOCATION}\"}\n",
        "      }\n",
        "\n",
        "    return label_json\n",
        "\n",
        "\n",
        "def get_scan_api_endpoint(scan_operation_type,scan_id):\n",
        "    \"\"\"\n",
        "    Returns the scan API endpoint\n",
        "\n",
        "    Args:\n",
        "        scan_operation_type (str): Type of operation (CREATE_SCAN/RUN_SCAN)\n",
        "        scan_id (str): Scan id\n",
        "\n",
        "    Returns:\n",
        "        string: API endpoint\n",
        "    \"\"\"\n",
        "    scan_api_endpoint=\"\"\n",
        "\n",
        "    if(scan_operation_type == \"CREATE_SCAN\"):\n",
        "        scan_api_endpoint=f\"{DATA_SCAN_API_CREATE_ENDPOINT_PREFIX}{scan_id}\"\n",
        "    elif (scan_operation_type == \"LIST_SCAN\"):\n",
        "        scan_api_endpoint=f\"{DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX}{scan_id}\"\n",
        "    else:  # run scan\n",
        "        scan_api_endpoint=f\"{DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX}{scan_id}:run\"\n",
        "\n",
        "\n",
        "    return scan_api_endpoint\n",
        "\n",
        "def check_if_scan_already_exists(access_token,scan_id):\n",
        "    \"\"\"\n",
        "    Calls the BigQuery Scan API to check if a Data Scan already exists.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_api_endpoint (str): API endpoint\n",
        "        scan_id (str): Scan name\n",
        "\n",
        "    Returns:\n",
        "        string: NOT_FOUND or EXISTS_ALREADY\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if not access_token:\n",
        "        print(\"Access token is missing. Cannot proceed with API call.\")\n",
        "        return\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "\n",
        "    request_body = {}\n",
        "    scan_list_api_endpoint= DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX + scan_id\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = requests.get(scan_list_api_endpoint, headers=headers, json=request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        response_json = response.json()\n",
        "\n",
        "        print(f\"\\nAPI Call Successful! Status Code: {response.status_code}\")\n",
        "\n",
        "\n",
        "        # Check for status node - if it is found, it says  \"status\": \"NOT_FOUND\" - it means the scan does not exist\n",
        "        not_found = response_json['status']\n",
        "\n",
        "        if not_found:\n",
        "            return \"NOT_FOUND\"\n",
        "        else:\n",
        "            return \"EXISTS_ALREADY\"\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "        return \"NOT_FOUND\"\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "        return \"NOT_FOUND2\"\n",
        "\n",
        "\n",
        "\n",
        "def create_scan_synchronous(access_token,dataset_id, scan_id, scan_type, source_table_nm, scan_results_table_nm):\n",
        "    \"\"\"\n",
        "    Calls the BigQuery Scan API with the generated access token.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_id (str): Scan id\n",
        "        scan_type (str): DATA_PROFILE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN\n",
        "        source_table_nm (str): Name of source table in BQ\n",
        "        scan_results_table_nm (str): Name of profile results table in BQ\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        print(\"Access token is missing. Cannot proceed with API call.\")\n",
        "        return\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    #scan_list_status = check_if_scan_already_exists(access_token, scan_id)\n",
        "    scan_list_status = \"NOT_FOUND\"\n",
        "\n",
        "    if scan_list_status == \"NOT_FOUND\":\n",
        "\n",
        "        scan_request_body = generate_scan_request_body(dataset_id,scan_id, scan_type, source_table_nm, scan_results_table_nm)\n",
        "        scan_create_api_endpoint = get_scan_api_endpoint(\"CREATE_SCAN\",scan_id)\n",
        "        print(f\"\\nCalling API: {scan_create_api_endpoint}\")\n",
        "        print(f\"Request Body: {json.dumps(scan_request_body, indent=2)}\")\n",
        "\n",
        "        try:\n",
        "            # The ':run' method is a POST request. It returns a Long Running Operation (LRO).\n",
        "            response = requests.post(scan_create_api_endpoint, headers=headers, json=scan_request_body)\n",
        "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "            initial_response = response.json()\n",
        "            # Extract the operation name to poll\n",
        "            operation_name = initial_response.get(\"name\")\n",
        "            if operation_name:\n",
        "\n",
        "                # Poll for the completion of the operation\n",
        "                final_result = poll_data_scan_operation(\"CREATE_SCAN\",operation_name, access_token)\n",
        "                if final_result:\n",
        "                    print(f\"Scan successfully completed and results obtained. Operation name: {operation_name}\")\n",
        "                else:\n",
        "                    print(f\"Scan operation did not complete successfully or timed out. Operation name: {operation_name}\")\n",
        "            else:\n",
        "                print(\"Could not find 'name' in the initial response. Cannot poll for completion.\")\n",
        "\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            print(f\"HTTP error occurred: {http_err}\")\n",
        "            print(f\"Response Body: {response.text}\")\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            print(f\"An error occurred during the API call: {req_err}\")\n",
        "    else:\n",
        "        print(\"Scan already exists; Skipping creation..\")\n",
        "\n",
        "def poll_data_scan_operation(operation_type: str, operation_name: str, access_token: str, poll_interval_seconds=30, timeout_minutes=30):\n",
        "    \"\"\"\n",
        "    Polls the Dataplex Operation API to check for the completion of a data scan.\n",
        "\n",
        "    Args:\n",
        "        operation_name (str): The full resource name of the LRO (Long Running Operation)\n",
        "                              returned by the data scan run API call.\n",
        "        access_token (str): The Google Cloud access token.\n",
        "        poll_interval_seconds (int): How often to poll the API, in seconds.\n",
        "        timeout_minutes (int): Maximum time to wait for the operation to complete, in minutes.\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    # Base URL for Google Cloud Long Running Operations API\n",
        "    # Example operation_name: projects/PROJECT_ID/locations/LOCATION/operations/OPERATION_ID\n",
        "    # We need to ensure the base URL matches how the operation_name is structured for the API call.\n",
        "    # The operation_name already contains the full path, so we use it directly.\n",
        "    operation_api_url = f\"https://dataplex.googleapis.com/v1/{operation_name}\"\n",
        "\n",
        "    print(f\"\\nPolling operation: {operation_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    while (time.time() - start_time) < (timeout_minutes * 60):\n",
        "        try:\n",
        "            response = requests.get(operation_api_url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            operation_status = response.json()\n",
        "\n",
        "\n",
        "            if(operation_type == \"CREATE_SCAN\"):\n",
        "                if operation_status.get(\"done\"):\n",
        "                    print(f\"Operation {operation_name} completed.\")\n",
        "                    if \"error\" in operation_status:\n",
        "                        print(f\"Operation failed with error: {operation_status['error']}\")\n",
        "                        return None\n",
        "                    elif \"response\" in operation_status:\n",
        "                        print(f\"Operation succeeded. Result: {json.dumps(operation_status['response'], indent=2)}\")\n",
        "                        return operation_status[\"response\"]\n",
        "                    else:\n",
        "                        print(\"Operation finished, but no explicit response or error found.\")\n",
        "                        return operation_status # Return the full status for further inspection\n",
        "                else:\n",
        "                    print(f\"Operation {operation_name} still running. Retrying in {poll_interval_seconds} seconds...\")\n",
        "                    time.sleep(poll_interval_seconds)\n",
        "\n",
        "            else:\n",
        "                if operation_status[\"state\"].upper()==\"COMPLETED\" or operation_status[\"state\"].upper()==\"SUCCEEDED\" or operation_status[\"state\"].upper()==\"DONE\":\n",
        "                    print(f\"Operation {operation_name} completed.\")\n",
        "                    if \"error\" in operation_status:\n",
        "                        print(f\"Operation failed with error: {operation_status['error']}\")\n",
        "                        return None\n",
        "                    elif \"response\" in operation_status:\n",
        "                        print(f\"Operation succeeded. Result: {json.dumps(operation_status['response'], indent=2)}\")\n",
        "                        return operation_status[\"response\"]\n",
        "                    else:\n",
        "                        print(\"Operation finished, but no explicit response or error found.\")\n",
        "                        return operation_status # Return the full status for further inspection\n",
        "                else:\n",
        "                    print(f\"Operation {operation_name} still running. Retrying in {poll_interval_seconds} seconds...\")\n",
        "                    time.sleep(poll_interval_seconds)\n",
        "\n",
        "\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            print(f\"HTTP error occurred during polling: {http_err}\")\n",
        "            print(f\"Response Body: {response.text}\")\n",
        "            return None\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            print(f\"An error occurred during polling the operation: {req_err}\")\n",
        "            return None\n",
        "\n",
        "    print(f\"Polling timed out after {timeout_minutes} minutes for operation: {operation_name}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def run_scan_synchronous(access_token, scan_id):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to run a previously created Scan and polls for its completion.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_api_endpoint (str): API endpoint\n",
        "        scan_name (str): Name of the precreated scan\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "\n",
        "    if not access_token:\n",
        "        print(\"Access token is missing. Cannot proceed with API call.\")\n",
        "        return\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    # The request body for a ':run' operation is typically empty for on-demand execution.\n",
        "    request_body = {}\n",
        "\n",
        "    scan_run_api_endpoint = get_scan_api_endpoint(\"RUN_SCAN\",scan_id)\n",
        "\n",
        "    print(f\"\\nAttempting to run scan: {scan_id} at {scan_run_api_endpoint}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # The ':run' method is a POST request. It returns a Long Running Operation (LRO).\n",
        "        response = requests.post(scan_run_api_endpoint, headers=headers, json=request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        initial_response = response.json()\n",
        "\n",
        "        # Extract the operation name to poll\n",
        "        operation_name = initial_response['job']['name']\n",
        "        #initial_response.get(\"job.name\")\n",
        "        if operation_name:\n",
        "            # Poll for the completion of the operation\n",
        "            final_result = poll_data_scan_operation(\"RUN_SCAN\",operation_name, access_token)\n",
        "            if final_result:\n",
        "                print(f\"\\nScan successfully completed and results obtained. Operation name: {operation_name}\")\n",
        "                print(final_result)\n",
        "            else:\n",
        "                print(f\"\\nScan operation did not complete successfully or timed out. Operation name: {operation_name}\")\n",
        "        else:\n",
        "            print(\"Could not find 'name' in the initial response. Cannot poll for completion.\")\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "\n",
        "def run_scan_async(access_token, scan_id):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to run a previously created Scan and polls for its completion.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_api_endpoint (str): API endpoint\n",
        "        scan_name (str): Name of the precreated scan\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "\n",
        "    if not access_token:\n",
        "        print(\"Access token is missing. Cannot proceed with API call.\")\n",
        "        return\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    # The request body for a ':run' operation is typically empty for on-demand execution.\n",
        "    request_body = {}\n",
        "\n",
        "    scan_run_api_endpoint = get_scan_api_endpoint(\"RUN_SCAN\",scan_id)\n",
        "\n",
        "    print(f\"\\nAttempting to run scan: {scan_id} at {scan_run_api_endpoint}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # The ':run' method is a POST request. It returns a Long Running Operation (LRO).\n",
        "        response = requests.post(scan_run_api_endpoint, headers=headers, json=request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        initial_response = response.json()\n",
        "        print(f\"initial_response: {initial_response}\")\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "\n",
        "\n",
        "def list_scans(access_token, scan_type):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to list data scans, optionally filtering by type.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_type (str): DATA_PROFILE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of the scan list if successful, or an error\n",
        "             message string.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        msg =\"Access token is missing. Cannot proceed with API call.\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    scan_list_api_endpoint = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans\"\n",
        "\n",
        "\n",
        "    scan_type_map = {\n",
        "        \"DATA_PROFILE_SCAN\": \"DATA_PROFILE\",\n",
        "        \"DATA_DOCUMENTATION_SCAN\": \"DATA_DOCUMENTATION\",\n",
        "        \"DATA_KNOWLEDGE_ENGINE_SCAN\": \"KNOWLEDGE_ENGINE\"\n",
        "    }\n",
        "    params = {}\n",
        "\n",
        "\n",
        "    if scan_type != \"ALL\":\n",
        "       api_scan_type = scan_type_map.get(scan_type)\n",
        "       if api_scan_type:\n",
        "           params['filter'] = f'type=\"{api_scan_type}\"'\n",
        "       scan_list_api_endpoint = f\"{scan_list_api_endpoint}?{urlencode(params)}\"\n",
        "\n",
        "       if api_scan_type == scan_type_map.get(\"DATA_KNOWLEDGE_ENGINE_SCAN\"):\n",
        "        markdown_table = \"| Dataset |  Scan |  State | \\n\"\n",
        "        markdown_table += \"|---|---|---|\\n\"\n",
        "       else:\n",
        "        markdown_table = \"| Dataset | Table | Scan |  State | \\n\"\n",
        "        markdown_table += \"|---|---|---|---|\\n\"\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = requests.get(scan_list_api_endpoint, headers=headers)\n",
        "        response.raise_for_status()  # Raises an error for bad status codes (4xx or 5xx)\n",
        "        response_json = response.json()\n",
        "\n",
        "        if \"dataScans\" in response_json and response_json[\"dataScans\"]:\n",
        "            for scan in response_json[\"dataScans\"]:\n",
        "                table_resource_uri = scan.get('data', {}).get('resource', '')\n",
        "                table_resource_uri_parts = table_resource_uri.split(\"/\")\n",
        "                source_dataset_id = table_resource_uri_parts[6]\n",
        "\n",
        "                if(source_dataset_id in SOURCE_BQ_DATASETS_IN_SCOPE):\n",
        "\n",
        "                    if api_scan_type == scan_type_map.get(\"DATA_KNOWLEDGE_ENGINE_SCAN\"):\n",
        "                        markdown_table += (f\"| {source_dataset_id} |  {scan.get('displayName', 'N/A')} |  {scan.get('state', 'N/A')} | \\n\")\n",
        "                    else:\n",
        "\n",
        "                        if len(table_resource_uri_parts) >= 8:\n",
        "\n",
        "                            source_table_id = table_resource_uri_parts[8]\n",
        "                            markdown_table += (f\"| {source_dataset_id} | {source_table_id} | {scan.get('displayName', 'N/A')} |  {scan.get('state', 'N/A')} | \\n\")\n",
        "\n",
        "            return \"\\n\".join(markdown_table)\n",
        "        else:\n",
        "            print(\"No data scans found in this location.\")\n",
        "            return \"No data scans found in this location.\"\n",
        "\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP Error: {e.response.status_code} - {e.response.text}\")\n",
        "        error_message = f\"HTTP Error: {e.response.status_code} - {e.response.text}\"\n",
        "        return error_message\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        error_message = f\"An unexpected error occurred: {e}\"\n",
        "        return error_message\n",
        "\n",
        "def list_scan_jobs(access_token: str, scan_id: str):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to list data scan jobs, optionally filtering by type.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_id (str): scan name\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of the scan job list in table markdown format if successful, or an error\n",
        "             message string.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        msg =\"Access token is missing. Cannot proceed with API call.\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    scan_job_list_api_endpoint = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans/{scan_id}/jobs\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(scan_job_list_api_endpoint, headers=headers)\n",
        "        response.raise_for_status()  # Raises an error for bad status codes (4xx or 5xx)\n",
        "        response_json = response.json()\n",
        "\n",
        "        markdown_table = \"| Job_Name | UID | State | Start_Time | End_Time |\\n\"\n",
        "        markdown_table += \"|---|---|---|---|---|\\n\"\n",
        "\n",
        "\n",
        "        if \"dataScanJobs\" in response_json and response_json[\"dataScanJobs\"]:\n",
        "            for job in response_json[\"dataScanJobs\"]:\n",
        "                markdown_table += (f\"| {job['name'].split('/')[-1]} | {job.get('uid', 'N/A')} | {job.get('state', 'N/A')} |  {job.get('startTime', 'N/A')} | {job.get('endTime', 'N/A')} | \\n\")\n",
        "\n",
        "            return \"\\n\".join(markdown_table)\n",
        "\n",
        "        else:\n",
        "            print(\"No jobs found for this data scan.\")\n",
        "\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP Error: {e.response.status_code} - {e.response.text}\")\n",
        "        error_message = f\"HTTP Error: {e.response.status_code} - {e.response.text}\"\n",
        "        return error_message\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        error_message = f\"An unexpected error occurred: {e}\"\n",
        "        return error_message\n",
        "\n",
        "\n",
        "def fetch_scan_results(access_token: str, scan_id: str):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to list data scan results.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_id (str): scan name\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of the results including markdown tables where applicable\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        msg =\"Access token is missing. Cannot proceed with API call.\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    scan_results_list_api_endpoint = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans/{scan_id}?view=FULL\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = requests.get(scan_results_list_api_endpoint, headers=headers)\n",
        "        response.raise_for_status()  # Raises an error for bad status codes (4xx or 5xx)\n",
        "        response_json = response.json()\n",
        "        return response_json\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP Error: {e.response.status_code} - {e.response.text}\")\n",
        "        try:\n",
        "            # Return JSON error if available, otherwise raw text\n",
        "            return e.response.json()\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Failed to decode JSON from response: {e}\")\n",
        "            return {\"error\": e.response.text}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Failed to decode JSON from response: {e}\")\n",
        "        return {\"error\": f\"Failed to decode JSON from response: {e}\"}\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return {\"error\": f\"An unexpected error occurred: {e}\"}\n",
        "\n",
        "\n",
        "def persist_documentation_scan_table_metadata(access_token: str, scan_id: str):\n",
        "    \"\"\"\n",
        "    Persists the table metadata generated by the data documentation scan .\n",
        "\n",
        "    Args:\n",
        "        access_token: token\n",
        "        scan_id: scan id\n",
        "\n",
        "    Returns:\n",
        "        A string indicating the status\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        msg =\"Access token is missing. Cannot proceed with API call.\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "\n",
        "    scan_result = fetch_scan_results(access_token, scan_id)\n",
        "\n",
        "    # Extract info from scan result\n",
        "    table_resource_uri = scan_result[\"data\"][\"resource\"]\n",
        "    updated_table_description = scan_result[\"dataDocumentationResult\"][\"overview\"]\n",
        "    updated_schema_with_column_descriptions = scan_result[\"dataDocumentationResult\"][\"schema\"]\n",
        "\n",
        "    # Check for errors from the API call\n",
        "    if not isinstance(scan_result, dict) or \"error\" in scan_result:\n",
        "        print(f\"Error fetching scan results: {scan_result}\")\n",
        "        return f\"Failed to fetch or parse scan results: {scan_result}\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Parse table URI\n",
        "        # e.g., //bigquery.googleapis.com/projects/PROJECT_ID/datasets/DATASET_ID/tables/TABLE_ID\n",
        "        parts = table_resource_uri.split(\"/\")\n",
        "        project_id = parts[4]\n",
        "        dataset_id = parts[6]\n",
        "        table_id = parts[8]\n",
        "\n",
        "        # Get BQ client and table\n",
        "        bq_client = get_bq_client()\n",
        "        if not bq_client:\n",
        "            return \"Failed to get BigQuery client.\"\n",
        "\n",
        "        table_ref = bq_client.dataset(dataset_id, project=project_id).table(\n",
        "            table_id\n",
        "        )\n",
        "        table = bq_client.get_table(table_ref)\n",
        "\n",
        "\n",
        "        # Update table description\n",
        "        table.description = updated_table_description\n",
        "        existing_table_schema = table.schema\n",
        "\n",
        "\n",
        "        # Update column descriptions by creating a new schema\n",
        "        updated_schema = []\n",
        "        for existing_field in existing_table_schema:\n",
        "\n",
        "            for item in updated_schema_with_column_descriptions[\"fields\"]:\n",
        "                if item[\"name\"] == existing_field.name:\n",
        "                    updated_field = bigquery.SchemaField(\n",
        "                    existing_field.name,\n",
        "                    existing_field.field_type,\n",
        "                    existing_field.mode,\n",
        "                    description=item[\"description\"],\n",
        "                    )\n",
        "                    updated_schema.append(updated_field)\n",
        "\n",
        "\n",
        "\n",
        "        table.schema = updated_schema\n",
        "\n",
        "        # Update the table\n",
        "        bq_client.update_table(table, [\"description\", \"schema\"])\n",
        "\n",
        "        print(\n",
        "            \"Successfully updated metadata for table \"\n",
        "            f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "        )\n",
        "        return \"Succeeded\"\n",
        "\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"Error parsing scan result: {e}\")\n",
        "        return f\"Failed to parse scan result: {e}\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "\n",
        "def patch_source_dataset_with_labels(access_token, dataset_id,scan_type, scan_id):\n",
        "    \"\"\"\n",
        "    Patches the source BigQuery dataset with labels to correlate with the data scans\n",
        "\n",
        "    Args:\n",
        "        access_token (str): The Google Cloud access token\n",
        "        dataset_id (str): The dataset id\n",
        "        scan_type (str):\n",
        "        scan_id (str): The scan id\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "            msg=\"Access token is missing. Cannot proceed with API call.\"\n",
        "            return msg\n",
        "\n",
        "    API_ENDPOINT=f\"https://bigquery.googleapis.com/bigquery/v2/projects/{PROJECT_ID}/datasets/{dataset_id}?\"\n",
        "\n",
        "    patch_request_body= generate_patch_label_request_body(scan_type,scan_id)\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "\n",
        "    # Apply the patch\n",
        "    try:\n",
        "        response = requests.patch(API_ENDPOINT, headers=headers, json=patch_request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")"
      ],
      "metadata": {
        "id": "sEu0MNxcOkQ-"
      },
      "id": "sEu0MNxcOkQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Variables/Configs"
      ],
      "metadata": {
        "id": "f3BD-_1dQEYD"
      },
      "id": "f3BD-_1dQEYD"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID_LIST=!gcloud config list --format \"value(core.project)\" 2>/dev/null\n",
        "PROJECT_ID=PROJECT_ID_LIST[0]\n",
        "LOCATION=\"us-central1\"\n",
        "OLTP_DATASET_ID=\"rscw_ds\"\n",
        "DATA_SCAN_API_CREATE_ENDPOINT_PREFIX=f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans?dataScanId=\"\n",
        "DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX=(f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans/\")\n",
        "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n",
        "SOURCE_BQ_DATASETS_IN_SCOPE=\"rscw_ds\"\n",
        "SCAN_RESULTS_BQ_DATASET_ID=\"rscw_ops_ds\"\n",
        "\n",
        "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
        "print(f\"LOCATION: {PROJECT_ID}\")\n",
        "print(f\"OLTP_DATASET_ID: {OLTP_DATASET_ID}\")\n",
        "print(f\"DATA_SCAN_API_CREATE_ENDPOINT_PREFIX: {DATA_SCAN_API_CREATE_ENDPOINT_PREFIX}\")\n",
        "print(f\"DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX: {DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX}\")\n"
      ],
      "metadata": {
        "id": "KZC1PkFwQIrA"
      },
      "id": "KZC1PkFwQIrA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Scan Utils"
      ],
      "metadata": {
        "id": "zNaPD44hO97S"
      },
      "id": "zNaPD44hO97S"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json, time, logging\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import GoogleAPIError\n",
        "from google.api_core.exceptions import NotFound\n",
        "from google.api_core import exceptions\n",
        "from urllib.parse import urlencode # Import urlencode\n",
        "\n",
        "\n",
        "\n",
        "def get_access_token():\n",
        "    \"\"\"\n",
        "    Generates an access token using Application Default Credentials or a service account key file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Authenticate using Application Default Credentials (ADC)\n",
        "        # This will automatically find credentials set via `gcloud auth application-default login`\n",
        "        # or from the environment if running on GCP.\n",
        "        credentials, project = google.auth.default(scopes=SCOPES)\n",
        "\n",
        "        # Refresh the credentials to ensure an up-to-date access token\n",
        "        request = google.auth.transport.requests.Request()\n",
        "        credentials.refresh(request)\n",
        "        return credentials.token\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating access token: {e}\")\n",
        "        return None\n",
        "\n",
        "def patch_source_table_with_labels(access_token, dataset_id,scan_type, scan_id,  source_table_nm):\n",
        "    \"\"\"\n",
        "    Patches the source BigQuery table with labels to correlate with the data scans\n",
        "\n",
        "    Args:\n",
        "        access_token (str): The Google Cloud access token\n",
        "        dataset_id (str): The dataset id\n",
        "        scan_type (str):\n",
        "        scan_id (str): The scan id\n",
        "        source_table_nm (str): The source table name\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "            msg=\"Access token is missing. Cannot proceed with API call.\"\n",
        "            return msg\n",
        "\n",
        "    API_ENDPOINT=f\"https://bigquery.googleapis.com/bigquery/v2/projects/{PROJECT_ID}/datasets/{dataset_id}/tables/{source_table_nm}?\"\n",
        "\n",
        "    patch_request_body= generate_patch_label_request_body(scan_type,scan_id)\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "\n",
        "    # Apply the patch\n",
        "    try:\n",
        "        response = requests.patch(API_ENDPOINT, headers=headers, json=patch_request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "\n",
        "\n",
        "def generate_scan_request_body(dataset_id,scan_id, scan_type,\n",
        "                                            source_table_nm, profile_results_table_nm):\n",
        "    \"\"\"\n",
        "    Generates the scan request body\n",
        "\n",
        "    Args:\n",
        "        scan_id (str): scan id\n",
        "        scan_type (str): Type of scan (DATA_PROFLE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN)\n",
        "        source_table_nm (str): Source table name\n",
        "        profile_results_table_nm (str): The profile results table name\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        string: JSON with the request body for the data scan API call\n",
        "    \"\"\"\n",
        "    scan_request_body={}\n",
        "    if(scan_type == \"DATA_PROFILE_SCAN\"):\n",
        "\n",
        "        scan_request_body={\n",
        "            \"displayName\": f\"{scan_id}\",\n",
        "            \"data\": {\n",
        "                \"resource\": f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{dataset_id}/tables/{source_table_nm}\"\n",
        "            },\n",
        "            \"dataProfileSpec\": {\n",
        "                \"postScanActions\":\n",
        "                {\n",
        "                    \"bigqueryExport\":\n",
        "                    {\n",
        "                        \"resultsTable\": f\"projects/{PROJECT_ID}/datasets/{SCAN_RESULTS_BQ_DATASET_ID}/tables/{profile_results_table_nm}\"\n",
        "                    }\n",
        "                    }\n",
        "\n",
        "            },\n",
        "            \"executionSpec\": {\n",
        "                \"trigger\": {\n",
        "                    \"onDemand\": {} # Run on demand for this example\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    elif(scan_type == \"DATA_DOCUMENTATION_SCAN\"):\n",
        "        scan_request_body={\n",
        "        \"displayName\": f\"{scan_id}\",\n",
        "        \"type\": \"DATA_DOCUMENTATION\",\n",
        "        \"dataDocumentationSpec\": {},\n",
        "        \"data\": {\n",
        "            \"resource\": f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{dataset_id}/tables/{source_table_nm}\"\n",
        "        },\n",
        "        \"executionSpec\": {\n",
        "            \"trigger\": {\n",
        "                \"onDemand\": {} # Run on demand for this example\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    elif(scan_type == \"DATA_KNOWLEDGE_ENGINE_SCAN\"):\n",
        "        scan_request_body={\n",
        "        \"displayName\": f\"{scan_id}\",\n",
        "        \"type\": \"DATA_DOCUMENTATION\",\n",
        "        \"dataDocumentationSpec\": {},\n",
        "        \"data\": {\n",
        "            \"resource\": f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{dataset_id}\"\n",
        "        },\n",
        "        \"executionSpec\": {\n",
        "            \"trigger\": {\n",
        "                \"onDemand\": {} # Run on demand for this example\n",
        "            }\n",
        "        },\n",
        "\n",
        "    }\n",
        "\n",
        "    return scan_request_body\n",
        "\n",
        "def generate_patch_label_request_body(scan_type, scan_id):\n",
        "    \"\"\"\n",
        "        Returns the patch labels json that needs to be attached to the source table to tie programmatic scans to the UI\n",
        "\n",
        "        Args:\n",
        "            scan_type (str): Type of scan (DATA_PROFLE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN)\n",
        "            scan_id (str): Scan id\n",
        "            operation_type (str): Type of operation (CREATE_SCAN/RUN_SCAN)\n",
        "\n",
        "        Returns:\n",
        "            string: json with the patch labels\n",
        "        \"\"\"\n",
        "    label_json=\"\"\n",
        "    scan_stub = \"\"\n",
        "    if scan_type == \"DATA_PROFILE_SCAN\":\n",
        "        scan_stub=\"dp\"\n",
        "    elif scan_type == \"DATA_DOCUMENTATION_SCAN\":\n",
        "        scan_stub=\"data-documentation\"\n",
        "    elif scan_type == \"DATA_KNOWLEDGE_ENGINE_SCAN\":\n",
        "        scan_stub=\"knowledge-engine\"\n",
        "\n",
        "\n",
        "    label_json = {\n",
        "        \"labels\": {f\"dataplex-{scan_stub}-published-scan\":f\"{scan_id}\",\n",
        "                 f\"dataplex-{scan_stub}-published-project\":f\"{PROJECT_ID}\",\n",
        "                 f\"dataplex-{scan_stub}-published-location\":f\"{LOCATION}\"}\n",
        "      }\n",
        "\n",
        "    return label_json\n",
        "\n",
        "\n",
        "def get_scan_api_endpoint(scan_operation_type,scan_id):\n",
        "    \"\"\"\n",
        "    Returns the scan API endpoint\n",
        "\n",
        "    Args:\n",
        "        scan_operation_type (str): Type of operation (CREATE_SCAN/RUN_SCAN)\n",
        "        scan_id (str): Scan id\n",
        "\n",
        "    Returns:\n",
        "        string: API endpoint\n",
        "    \"\"\"\n",
        "    scan_api_endpoint=\"\"\n",
        "\n",
        "    if(scan_operation_type == \"CREATE_SCAN\"):\n",
        "        scan_api_endpoint=f\"{DATA_SCAN_API_CREATE_ENDPOINT_PREFIX}{scan_id}\"\n",
        "    elif (scan_operation_type == \"LIST_SCAN\"):\n",
        "        scan_api_endpoint=f\"{DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX}{scan_id}\"\n",
        "    else:  # run scan\n",
        "        scan_api_endpoint=f\"{DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX}{scan_id}:run\"\n",
        "\n",
        "\n",
        "    return scan_api_endpoint\n",
        "\n",
        "def check_if_scan_already_exists(access_token,scan_id):\n",
        "    \"\"\"\n",
        "    Calls the BigQuery Scan API to check if a Data Scan already exists.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_api_endpoint (str): API endpoint\n",
        "        scan_id (str): Scan name\n",
        "\n",
        "    Returns:\n",
        "        string: NOT_FOUND or EXISTS_ALREADY\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if not access_token:\n",
        "        print(\"Access token is missing. Cannot proceed with API call.\")\n",
        "        return\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "\n",
        "    request_body = {}\n",
        "    scan_list_api_endpoint= DATA_SCAN_API_EXECUTION_ENDPOINT_PREFIX + scan_id\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = requests.get(scan_list_api_endpoint, headers=headers, json=request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        response_json = response.json()\n",
        "\n",
        "        print(f\"\\nAPI Call Successful! Status Code: {response.status_code}\")\n",
        "\n",
        "\n",
        "        # Check for status node - if it is found, it says  \"status\": \"NOT_FOUND\" - it means the scan does not exist\n",
        "        not_found = response_json['status']\n",
        "\n",
        "        if not_found:\n",
        "            return \"NOT_FOUND\"\n",
        "        else:\n",
        "            return \"EXISTS_ALREADY\"\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "        return \"NOT_FOUND\"\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "        return \"NOT_FOUND2\"\n",
        "\n",
        "\n",
        "\n",
        "def create_scan_synchronous(access_token,dataset_id, scan_id, scan_type, source_table_nm, scan_results_table_nm):\n",
        "    \"\"\"\n",
        "    Calls the BigQuery Scan API with the generated access token.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_id (str): Scan id\n",
        "        scan_type (str): DATA_PROFILE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN\n",
        "        source_table_nm (str): Name of source table in BQ\n",
        "        scan_results_table_nm (str): Name of profile results table in BQ\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        print(\"Access token is missing. Cannot proceed with API call.\")\n",
        "        return\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    scan_list_status = check_if_scan_already_exists(access_token, scan_id)\n",
        "\n",
        "    if scan_list_status == \"NOT_FOUND\":\n",
        "\n",
        "        scan_request_body = generate_scan_request_body(dataset_id,scan_id, scan_type, source_table_nm, scan_results_table_nm)\n",
        "        scan_create_api_endpoint = get_scan_api_endpoint(\"CREATE_SCAN\",scan_id)\n",
        "        print(f\"\\nCalling API: {scan_create_api_endpoint}\")\n",
        "        print(f\"Request Body: {json.dumps(scan_request_body, indent=2)}\")\n",
        "\n",
        "        try:\n",
        "            # The ':run' method is a POST request. It returns a Long Running Operation (LRO).\n",
        "            response = requests.post(scan_create_api_endpoint, headers=headers, json=scan_request_body)\n",
        "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "            initial_response = response.json()\n",
        "            # Extract the operation name to poll\n",
        "            operation_name = initial_response.get(\"name\")\n",
        "            if operation_name:\n",
        "\n",
        "                # Poll for the completion of the operation\n",
        "                final_result = poll_data_scan_operation(\"CREATE_SCAN\",operation_name, access_token)\n",
        "                if final_result:\n",
        "                    print(f\"Scan successfully completed and results obtained. Operation name: {operation_name}\")\n",
        "                else:\n",
        "                    print(f\"Scan operation did not complete successfully or timed out. Operation name: {operation_name}\")\n",
        "            else:\n",
        "                print(\"Could not find 'name' in the initial response. Cannot poll for completion.\")\n",
        "\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            print(f\"HTTP error occurred: {http_err}\")\n",
        "            print(f\"Response Body: {response.text}\")\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            print(f\"An error occurred during the API call: {req_err}\")\n",
        "    else:\n",
        "        print(\"Scan already exists; Skipping creation..\")\n",
        "\n",
        "def poll_data_scan_operation(operation_type: str, operation_name: str, access_token: str, poll_interval_seconds=30, timeout_minutes=30):\n",
        "    \"\"\"\n",
        "    Polls the Dataplex Operation API to check for the completion of a data scan.\n",
        "\n",
        "    Args:\n",
        "        operation_name (str): The full resource name of the LRO (Long Running Operation)\n",
        "                              returned by the data scan run API call.\n",
        "        access_token (str): The Google Cloud access token.\n",
        "        poll_interval_seconds (int): How often to poll the API, in seconds.\n",
        "        timeout_minutes (int): Maximum time to wait for the operation to complete, in minutes.\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    # Base URL for Google Cloud Long Running Operations API\n",
        "    # Example operation_name: projects/PROJECT_ID/locations/LOCATION/operations/OPERATION_ID\n",
        "    # We need to ensure the base URL matches how the operation_name is structured for the API call.\n",
        "    # The operation_name already contains the full path, so we use it directly.\n",
        "    operation_api_url = f\"https://dataplex.googleapis.com/v1/{operation_name}\"\n",
        "\n",
        "    print(f\"\\nPolling operation: {operation_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    while (time.time() - start_time) < (timeout_minutes * 60):\n",
        "        try:\n",
        "            response = requests.get(operation_api_url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            operation_status = response.json()\n",
        "\n",
        "\n",
        "            if(operation_type == \"CREATE_SCAN\"):\n",
        "                if operation_status.get(\"done\"):\n",
        "                    print(f\"Operation {operation_name} completed.\")\n",
        "                    if \"error\" in operation_status:\n",
        "                        print(f\"Operation failed with error: {operation_status['error']}\")\n",
        "                        return None\n",
        "                    elif \"response\" in operation_status:\n",
        "                        print(f\"Operation succeeded. Result: {json.dumps(operation_status['response'], indent=2)}\")\n",
        "                        return operation_status[\"response\"]\n",
        "                    else:\n",
        "                        print(\"Operation finished, but no explicit response or error found.\")\n",
        "                        return operation_status # Return the full status for further inspection\n",
        "                else:\n",
        "                    print(f\"Operation {operation_name} still running. Retrying in {poll_interval_seconds} seconds...\")\n",
        "                    time.sleep(poll_interval_seconds)\n",
        "\n",
        "            else:\n",
        "                if operation_status[\"state\"].upper()==\"COMPLETED\" or operation_status[\"state\"].upper()==\"SUCCEEDED\" or operation_status[\"state\"].upper()==\"DONE\":\n",
        "                    print(f\"Operation {operation_name} completed.\")\n",
        "                    if \"error\" in operation_status:\n",
        "                        print(f\"Operation failed with error: {operation_status['error']}\")\n",
        "                        return None\n",
        "                    elif \"response\" in operation_status:\n",
        "                        print(f\"Operation succeeded. Result: {json.dumps(operation_status['response'], indent=2)}\")\n",
        "                        return operation_status[\"response\"]\n",
        "                    else:\n",
        "                        print(\"Operation finished, but no explicit response or error found.\")\n",
        "                        return operation_status # Return the full status for further inspection\n",
        "                else:\n",
        "                    print(f\"Operation {operation_name} still running. Retrying in {poll_interval_seconds} seconds...\")\n",
        "                    time.sleep(poll_interval_seconds)\n",
        "\n",
        "\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            print(f\"HTTP error occurred during polling: {http_err}\")\n",
        "            print(f\"Response Body: {response.text}\")\n",
        "            return None\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            print(f\"An error occurred during polling the operation: {req_err}\")\n",
        "            return None\n",
        "\n",
        "    print(f\"Polling timed out after {timeout_minutes} minutes for operation: {operation_name}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def run_scan_synchronous(access_token, scan_id):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to run a previously created Scan and polls for its completion.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_api_endpoint (str): API endpoint\n",
        "        scan_name (str): Name of the precreated scan\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "\n",
        "    if not access_token:\n",
        "        print(\"Access token is missing. Cannot proceed with API call.\")\n",
        "        return\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    # The request body for a ':run' operation is typically empty for on-demand execution.\n",
        "    request_body = {}\n",
        "\n",
        "    scan_run_api_endpoint = get_scan_api_endpoint(\"RUN_SCAN\",scan_id)\n",
        "\n",
        "    print(f\"\\nAttempting to run scan: {scan_id} at {scan_run_api_endpoint}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # The ':run' method is a POST request. It returns a Long Running Operation (LRO).\n",
        "        response = requests.post(scan_run_api_endpoint, headers=headers, json=request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        initial_response = response.json()\n",
        "\n",
        "        # Extract the operation name to poll\n",
        "        operation_name = initial_response['job']['name']\n",
        "        #initial_response.get(\"job.name\")\n",
        "        if operation_name:\n",
        "            # Poll for the completion of the operation\n",
        "            final_result = poll_data_scan_operation(\"RUN_SCAN\",operation_name, access_token)\n",
        "            if final_result:\n",
        "                print(f\"\\nScan successfully completed and results obtained. Operation name: {operation_name}\")\n",
        "                print(final_result)\n",
        "            else:\n",
        "                print(f\"\\nScan operation did not complete successfully or timed out. Operation name: {operation_name}\")\n",
        "        else:\n",
        "            print(\"Could not find 'name' in the initial response. Cannot poll for completion.\")\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "\n",
        "def run_scan_async(access_token, scan_id):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to run a previously created Scan and polls for its completion.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_api_endpoint (str): API endpoint\n",
        "        scan_name (str): Name of the precreated scan\n",
        "\n",
        "    Returns:\n",
        "        dict: The final operation response if successful, None if timed out or failed.\n",
        "    \"\"\"\n",
        "\n",
        "    if not access_token:\n",
        "        print(\"Access token is missing. Cannot proceed with API call.\")\n",
        "        return\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    # The request body for a ':run' operation is typically empty for on-demand execution.\n",
        "    request_body = {}\n",
        "\n",
        "    scan_run_api_endpoint = get_scan_api_endpoint(\"RUN_SCAN\",scan_id)\n",
        "\n",
        "    print(f\"\\nAttempting to run scan: {scan_id} at {scan_run_api_endpoint}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # The ':run' method is a POST request. It returns a Long Running Operation (LRO).\n",
        "        response = requests.post(scan_run_api_endpoint, headers=headers, json=request_body)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        initial_response = response.json()\n",
        "        print(f\"initial_response: {initial_response}\")\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        print(f\"Response Body: {response.text}\")\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"An error occurred during the API call: {req_err}\")\n",
        "\n",
        "\n",
        "def list_scans(access_token, scan_type):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to list data scans, optionally filtering by type.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_type (str): DATA_PROFILE_SCAN/DATA_DOCUMENTATION_SCAN/DATA_KNOWLEDGE_ENGINE_SCAN\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of the scan list if successful, or an error\n",
        "             message string.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        msg =\"Access token is missing. Cannot proceed with API call.\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    scan_list_api_endpoint = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans\"\n",
        "\n",
        "\n",
        "    scan_type_map = {\n",
        "        \"DATA_PROFILE_SCAN\": \"DATA_PROFILE\",\n",
        "        \"DATA_DOCUMENTATION_SCAN\": \"DATA_DOCUMENTATION\",\n",
        "        \"DATA_KNOWLEDGE_ENGINE_SCAN\": \"KNOWLEDGE_ENGINE\"\n",
        "    }\n",
        "    params = {}\n",
        "\n",
        "\n",
        "    if scan_type != \"ALL\":\n",
        "       api_scan_type = scan_type_map.get(scan_type)\n",
        "       if api_scan_type:\n",
        "           params['filter'] = f'type=\"{api_scan_type}\"'\n",
        "       scan_list_api_endpoint = f\"{scan_list_api_endpoint}?{urlencode(params)}\"\n",
        "\n",
        "       if api_scan_type == scan_type_map.get(\"DATA_KNOWLEDGE_ENGINE_SCAN\"):\n",
        "        markdown_table = \"| Dataset |  Scan |  State | \\n\"\n",
        "        markdown_table += \"|---|---|---|\\n\"\n",
        "       else:\n",
        "        markdown_table = \"| Dataset | Table | Scan |  State | \\n\"\n",
        "        markdown_table += \"|---|---|---|---|\\n\"\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = requests.get(scan_list_api_endpoint, headers=headers)\n",
        "        response.raise_for_status()  # Raises an error for bad status codes (4xx or 5xx)\n",
        "        response_json = response.json()\n",
        "\n",
        "        if \"dataScans\" in response_json and response_json[\"dataScans\"]:\n",
        "            for scan in response_json[\"dataScans\"]:\n",
        "                table_resource_uri = scan.get('data', {}).get('resource', '')\n",
        "                table_resource_uri_parts = table_resource_uri.split(\"/\")\n",
        "                source_dataset_id = table_resource_uri_parts[6]\n",
        "\n",
        "                if(source_dataset_id in SOURCE_BQ_DATASETS_IN_SCOPE):\n",
        "\n",
        "                    if api_scan_type == scan_type_map.get(\"DATA_KNOWLEDGE_ENGINE_SCAN\"):\n",
        "                        markdown_table += (f\"| {source_dataset_id} |  {scan.get('displayName', 'N/A')} |  {scan.get('state', 'N/A')} | \\n\")\n",
        "                    else:\n",
        "\n",
        "                        if len(table_resource_uri_parts) >= 8:\n",
        "\n",
        "                            source_table_id = table_resource_uri_parts[8]\n",
        "                            markdown_table += (f\"| {source_dataset_id} | {source_table_id} | {scan.get('displayName', 'N/A')} |  {scan.get('state', 'N/A')} | \\n\")\n",
        "\n",
        "            return \"\\n\".join(markdown_table)\n",
        "        else:\n",
        "            print(\"No data scans found in this location.\")\n",
        "            return \"No data scans found in this location.\"\n",
        "\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP Error: {e.response.status_code} - {e.response.text}\")\n",
        "        error_message = f\"HTTP Error: {e.response.status_code} - {e.response.text}\"\n",
        "        return error_message\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        error_message = f\"An unexpected error occurred: {e}\"\n",
        "        return error_message\n",
        "\n",
        "def list_scan_jobs(access_token: str, scan_id: str):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to list data scan jobs, optionally filtering by type.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_id (str): scan name\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of the scan job list in table markdown format if successful, or an error\n",
        "             message string.\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        msg =\"Access token is missing. Cannot proceed with API call.\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    scan_job_list_api_endpoint = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans/{scan_id}/jobs\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(scan_job_list_api_endpoint, headers=headers)\n",
        "        response.raise_for_status()  # Raises an error for bad status codes (4xx or 5xx)\n",
        "        response_json = response.json()\n",
        "\n",
        "        markdown_table = \"| Job_Name | UID | State | Start_Time | End_Time |\\n\"\n",
        "        markdown_table += \"|---|---|---|---|---|\\n\"\n",
        "\n",
        "\n",
        "        if \"dataScanJobs\" in response_json and response_json[\"dataScanJobs\"]:\n",
        "            for job in response_json[\"dataScanJobs\"]:\n",
        "                markdown_table += (f\"| {job['name'].split('/')[-1]} | {job.get('uid', 'N/A')} | {job.get('state', 'N/A')} |  {job.get('startTime', 'N/A')} | {job.get('endTime', 'N/A')} | \\n\")\n",
        "\n",
        "            return \"\\n\".join(markdown_table)\n",
        "\n",
        "        else:\n",
        "            print(\"No jobs found for this data scan.\")\n",
        "\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP Error: {e.response.status_code} - {e.response.text}\")\n",
        "        error_message = f\"HTTP Error: {e.response.status_code} - {e.response.text}\"\n",
        "        return error_message\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        error_message = f\"An unexpected error occurred: {e}\"\n",
        "        return error_message\n",
        "\n",
        "\n",
        "def fetch_scan_results(access_token: str, scan_id: str):\n",
        "    \"\"\"\n",
        "    Calls the Dataplex API to list data scan results.\n",
        "\n",
        "    Args:\n",
        "        access_token (str): token\n",
        "        scan_id (str): scan name\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of the results including markdown tables where applicable\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        msg =\"Access token is missing. Cannot proceed with API call.\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    scan_results_list_api_endpoint = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/dataScans/{scan_id}?view=FULL\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = requests.get(scan_results_list_api_endpoint, headers=headers)\n",
        "        response.raise_for_status()  # Raises an error for bad status codes (4xx or 5xx)\n",
        "        response_json = response.json()\n",
        "        return response_json\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP Error: {e.response.status_code} - {e.response.text}\")\n",
        "        try:\n",
        "            # Return JSON error if available, otherwise raw text\n",
        "            return e.response.json()\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Failed to decode JSON from response: {e}\")\n",
        "            return {\"error\": e.response.text}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Failed to decode JSON from response: {e}\")\n",
        "        return {\"error\": f\"Failed to decode JSON from response: {e}\"}\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return {\"error\": f\"An unexpected error occurred: {e}\"}\n",
        "\n",
        "\n",
        "def persist_documentation_scan_table_metadata(access_token: str, scan_id: str):\n",
        "    \"\"\"\n",
        "    Persists the table metadata generated by the data documentation scan .\n",
        "\n",
        "    Args:\n",
        "        access_token: token\n",
        "        scan_id: scan id\n",
        "\n",
        "    Returns:\n",
        "        A string indicating the status\n",
        "    \"\"\"\n",
        "    if not access_token:\n",
        "        msg =\"Access token is missing. Cannot proceed with API call.\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "\n",
        "    scan_result = fetch_scan_results(access_token, scan_id)\n",
        "\n",
        "    # Extract info from scan result\n",
        "    table_resource_uri = scan_result[\"data\"][\"resource\"]\n",
        "    updated_table_description = scan_result[\"dataDocumentationResult\"][\"overview\"]\n",
        "    updated_schema_with_column_descriptions = scan_result[\"dataDocumentationResult\"][\"schema\"]\n",
        "\n",
        "    # Check for errors from the API call\n",
        "    if not isinstance(scan_result, dict) or \"error\" in scan_result:\n",
        "        print(f\"Error fetching scan results: {scan_result}\")\n",
        "        return f\"Failed to fetch or parse scan results: {scan_result}\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Parse table URI\n",
        "        # e.g., //bigquery.googleapis.com/projects/PROJECT_ID/datasets/DATASET_ID/tables/TABLE_ID\n",
        "        parts = table_resource_uri.split(\"/\")\n",
        "        project_id = parts[4]\n",
        "        dataset_id = parts[6]\n",
        "        table_id = parts[8]\n",
        "\n",
        "        # Get BQ client and table\n",
        "        bq_client = get_bq_client()\n",
        "        if not bq_client:\n",
        "            return \"Failed to get BigQuery client.\"\n",
        "\n",
        "        table_ref = bq_client.dataset(dataset_id, project=project_id).table(\n",
        "            table_id\n",
        "        )\n",
        "        table = bq_client.get_table(table_ref)\n",
        "\n",
        "\n",
        "        # Update table description\n",
        "        table.description = updated_table_description\n",
        "        existing_table_schema = table.schema\n",
        "\n",
        "\n",
        "        # Update column descriptions by creating a new schema\n",
        "        updated_schema = []\n",
        "        for existing_field in existing_table_schema:\n",
        "\n",
        "            for item in updated_schema_with_column_descriptions[\"fields\"]:\n",
        "                if item[\"name\"] == existing_field.name:\n",
        "                    updated_field = bigquery.SchemaField(\n",
        "                    existing_field.name,\n",
        "                    existing_field.field_type,\n",
        "                    existing_field.mode,\n",
        "                    description=item[\"description\"],\n",
        "                    )\n",
        "                    updated_schema.append(updated_field)\n",
        "\n",
        "\n",
        "\n",
        "        table.schema = updated_schema\n",
        "\n",
        "        # Update the table\n",
        "        bq_client.update_table(table, [\"description\", \"schema\"])\n",
        "\n",
        "        print(\n",
        "            \"Successfully updated metadata for table \"\n",
        "            f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "        )\n",
        "        return \"Succeeded\"\n",
        "\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"Error parsing scan result: {e}\")\n",
        "        return f\"Failed to parse scan result: {e}\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return f\"An unexpected error occurred: {e}\""
      ],
      "metadata": {
        "id": "McjnM1-fasYr"
      },
      "id": "McjnM1-fasYr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. BQ Utils"
      ],
      "metadata": {
        "id": "ZtC44VdTdsl_"
      },
      "id": "ZtC44VdTdsl_"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.api_core import exceptions\n",
        "import google.auth\n",
        "import itertools, json, logging\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "def get_bq_client() -> Optional[bigquery.Client]:\n",
        "  \"\"\"Initializes and returns a BigQuery client.\n",
        "\n",
        "  Returns:\n",
        "      Optional[bigquery.Client]: A BigQuery client instance, or None if\n",
        "      initialization fails.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    client = bigquery.Client(project=PROJECT_ID)\n",
        "    return client\n",
        "  except google.auth.exceptions.DefaultCredentialsError as e:\n",
        "    print(f\"Authentication failed: {e}\")\n",
        "    print(\n",
        "        \"Please configure your GCP credentials.\"\n",
        "        \"See https://cloud.google.com/docs/authentication/provide-credentials-adc\"\n",
        "    )\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred while creating BigQuery client: {e}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def execute_bq_sql_query(\n",
        "    sql_query: str\n",
        ") -> Optional[bigquery.table.RowIterator]:\n",
        "    \"\"\"Executes a SQL query and returns the results.\n",
        "\n",
        "    Args:\n",
        "        bq_client: The BigQuery client.\n",
        "        sql_query: The SQL query to execute.\n",
        "\n",
        "    Returns:\n",
        "        Optional[bigquery.table.RowIterator]: An iterator for the query\n",
        "        results, or None if an error occurs.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        bq_client = get_bq_client()\n",
        "\n",
        "        if not bq_client:\n",
        "            print(\"BigQuery client is not available.\")\n",
        "            return None\n",
        "\n",
        "        rows = bq_client.query_and_wait(sql_query)  # Make an API request.\n",
        "        return rows\n",
        "    except exceptions.GoogleAPICallError as e:\n",
        "        print(f\"BigQuery API call failed: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_markdown_table_from_bq_rows(row_iterator: bigquery.table.RowIterator):\n",
        "    \"\"\"Generates a Markdown table from a BigQuery row iterator.\"\"\"\n",
        "\n",
        "\n",
        "    if not row_iterator:\n",
        "        return \"No results to display.\"\n",
        "    try:\n",
        "        headers = [field.name for field in row_iterator.schema]\n",
        "\n",
        "        markdown_table = \"|\" + \"|\".join(headers) + \"|\\n\"\n",
        "        markdown_table += \"|\" + \"|\".join([\"---\"] * len(headers)) + \"|\\n\"\n",
        "\n",
        "        for row in row_iterator:\n",
        "            row_values = [str(row[header]) for header in headers]\n",
        "            markdown_table += \"|\" + \"|\".join(row_values) + \"|\\n\"\n",
        "\n",
        "        print(f\"Markdown Table: {markdown_table}\")\n",
        "\n",
        "        return markdown_table\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating markdown table: {e}\")\n",
        "        return \"Error generating markdown table.\"\n",
        "\n",
        "def get_query_results_markdown(sql_query: str) -> str:\n",
        "    \"\"\"Executes a SQL query and returns the results as a Markdown table.\"\"\"\n",
        "\n",
        "    rows = execute_bq_sql_query(sql_query)\n",
        "    if rows is None:\n",
        "        print(\"Failed to execute query and retrieve results.\")\n",
        "        return \"Failed to execute query and retrieve results.\"\n",
        "    else:\n",
        "        markdown_table = generate_markdown_table_from_bq_rows(rows)\n",
        "        return markdown_table\n",
        "\n",
        "def field_to_dict(field: bigquery.SchemaField) -> dict:\n",
        "        \"\"\"\n",
        "        Recursively convert a SchemaField into a dict, including subfields if any.\n",
        "        \"\"\"\n",
        "        field_dict = {\"name\": field.name, \"description\": field.description or \"\"}\n",
        "        # If the field is a RECORD with nested fields, recurse\n",
        "        if field.fields:\n",
        "            field_dict[\"fields\"] = [\n",
        "                field_to_dict(subfield) for subfield in field.fields\n",
        "            ]\n",
        "        return field_dict\n",
        "\n",
        "def fetch_all_tables_metadata_json() -> str:\n",
        "    \"\"\"\n",
        "    Retrieves detailed info about each table in the specified datasets and\n",
        "    returns it as a JSON string. For each table, it includes:\n",
        "      - table_name (full path in 'project.dataset.table')\n",
        "      - description\n",
        "      - columns (list of columns with name, description)\n",
        "        * handles nested fields (RECORD type) recursively\n",
        "    \"\"\"\n",
        "\n",
        "    table_iterators = []\n",
        "    project_id = PROJECT_ID\n",
        "\n",
        "    try:\n",
        "        bq_client = bigquery.Client(project=project_id)\n",
        "        bq_dataset_list = SOURCE_BQ_DATASETS_IN_SCOPE\n",
        "\n",
        "        for ds_id in bq_dataset_list:\n",
        "\n",
        "            try:\n",
        "                table_iterators.append(bq_client.list_tables(f\"{project_id}.{ds_id}\"))\n",
        "            except exceptions.NotFound:\n",
        "                print(f\"Dataset not found, skipping: {project_id}.{ds_id}\")\n",
        "    except google.auth.exceptions.DefaultCredentialsError as e:\n",
        "        print(f\"Authentication failed: {e}\")\n",
        "        return json.dumps({\"error\": f\"Authentication failed: {e}\"})\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during client setup: {e}\")\n",
        "        return json.dumps(\n",
        "            {\"error\": f\"An unexpected error occurred during client setup: {e}\"}\n",
        "        )\n",
        "\n",
        "    all_tables_info = []\n",
        "    for table_item in itertools.chain.from_iterable(table_iterators):\n",
        "        full_table_id = \"\"  # Initialize here for the except block\n",
        "        try:\n",
        "            full_table_id = (\n",
        "                f\"{table_item.project}.{table_item.dataset_id}.{table_item.table_id}\"\n",
        "            )\n",
        "\n",
        "            table_obj = bq_client.get_table(full_table_id)\n",
        "            table_info = {\n",
        "                \"table_name\": full_table_id,\n",
        "                \"description\": table_obj.description or \"\",\n",
        "                \"columns\": [field_to_dict(f) for f in table_obj.schema],\n",
        "            }\n",
        "            all_tables_info.append(table_info)\n",
        "        except exceptions.NotFound:\n",
        "            print(f\"Table not found, skipping: {full_table_id}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process table {full_table_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert the list of table dictionaries to a JSON string\n",
        "    return json.dumps(all_tables_info, indent=2)\n",
        "\n",
        "def fetch_bq_table_schema(table_fq_resource_uri: str) -> str:\n",
        "    \"\"\"\n",
        "    Retrieves the BQ table metadata\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "        table_resource_uri_parts = table_fq_resource_uri.split(\"/\")\n",
        "        full_table_id=table_resource_uri_parts[6] + \".\" + table_resource_uri_parts[8].rstrip('\"')\n",
        "        try:\n",
        "            table_obj = bq_client.get_table(full_table_id.strip())\n",
        "\n",
        "\n",
        "            serializable_schema = []\n",
        "            for field in table_obj.schema:\n",
        "                field_dict = {\n",
        "                    \"name\": field.name,\n",
        "                    \"type\": field.field_type,\n",
        "                    \"mode\": field.mode,\n",
        "                    \"description\": field.description,\n",
        "                }\n",
        "                if field.fields:  # Handle nested fields for RECORD types\n",
        "                    field_dict[\"fields\"] = [\n",
        "                        {\n",
        "                            \"name\": nested_field.name,\n",
        "                            \"type\": nested_field.field_type,\n",
        "                            \"mode\": nested_field.mode,\n",
        "                            \"description\": nested_field.description,\n",
        "                        }\n",
        "                        for nested_field in field.fields\n",
        "                    ]\n",
        "                serializable_schema.append(field_dict)\n",
        "\n",
        "            return json.dumps(serializable_schema, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "            return json.dumps({\"error\": f\"An unexpected error occurred: {e}\"}\n",
        "        )\n",
        "\n",
        "    except google.auth.exceptions.DefaultCredentialsError as e:\n",
        "        print(f\"Authentication failed: {e}\")\n",
        "        return json.dumps({\"error\": f\"Authentication failed: {e}\"})\n",
        "    except exceptions.NotFound:\n",
        "        print(f\"Resource not found: {table_fq_resource_uri}\")\n",
        "        return json.dumps(\n",
        "            {\"error\": \"Resource not found\"}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return json.dumps(\n",
        "            {\"error\": f\"An unexpected error occurred: {e}\"}\n",
        "        )\n",
        "\n",
        "def update_table_schema(project_id: str, dataset_id: str, table_id: str, new_schema: list[bigquery.SchemaField]):\n",
        "    \"\"\"\n",
        "    Updates the schema of a BigQuery table.\n",
        "\n",
        "    Args:\n",
        "        project_id: The project ID.\n",
        "        dataset_id: The dataset ID.\n",
        "        table_id: The table ID.\n",
        "        new_schema: A list of bigquery.SchemaField objects for the new schema.\n",
        "    \"\"\"\n",
        "\n",
        "    # Instantiates a BQ connection\n",
        "    try:\n",
        "        bq_client = bigquery.Client(project=project_id)\n",
        "        print(\"BigQuery client initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing BigQuery client: {e}\")\n",
        "        print(\"Please ensure you have authenticated with 'gcloud auth application-default login'\")\n",
        "        print(\"and that your PROJECT_ID is correct.\")\n",
        "        return\n",
        "\n",
        "    table_ref = bq_client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "    try:\n",
        "        table = bq_client.get_table(table_ref)\n",
        "        print(f\"Fetched table: {table.project}.{table.dataset_id}.{table.table_id}\")\n",
        "    except NotFound:\n",
        "        print(f\"Error: Table {dataset_id}.{table_id} not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while fetching the table: {e}\")\n",
        "        return\n",
        "\n",
        "    # Set the table's schema to the newly constructed schema.\n",
        "    table.schema = new_schema\n",
        "\n",
        "    # Make the API call to update the table's schema.\n",
        "    # The second argument to update_table() specifies which properties to update.\n",
        "    try:\n",
        "        table = bq_client.update_table(table, [\"schema\"])  # API request\n",
        "        print(f\"\\nSuccessfully updated the table schema for {table.table_id}.\")\n",
        "\n",
        "        #for field in table.schema:\n",
        "        #    print(f\" - {field.name} ({field.field_type}): {field.description}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred while updating the table schema: {e}\")\n",
        "\n",
        "\n",
        "def fetch_list_of_tables_in_dataset(dataset_id: str) :\n",
        "    \"\"\"Fetches a list of table IDs in a given BigQuery dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_id: The ID of the dataset.\n",
        "\n",
        "    Returns:\n",
        "        A list of table IDs, or an empty list if an error occurs.\n",
        "    \"\"\"\n",
        "    client = get_bq_client()\n",
        "    if not client:\n",
        "        print(\"Failed to initialize BigQuery client.\")\n",
        "        return [\"Failed to initialize BigQuery client.\"]\n",
        "\n",
        "    try:\n",
        "        bq_tables_list = client.list_tables(dataset_id)  # Make an API request.\n",
        "        return [table.table_id for table in bq_tables_list]\n",
        "    except exceptions.NotFound:\n",
        "        print(f\"Dataset not found: {dataset_id}\")\n",
        "        return [\"Dataset not found.\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while listing tables: {e}\")\n",
        "        return [\"An unexpected error occurred while listing tables.\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "yqBo3p0adwDE"
      },
      "id": "yqBo3p0adwDE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Start the data documentation scan on the dataset\n",
        "\n",
        "Note: <br>\n",
        "1. The scan will take 2-3 minutes to complete. Run this notebook and switch to the user manual.\n",
        "2. To start module 3, this notebook should have completed execution in entirety"
      ],
      "metadata": {
        "id": "jfRCzToMeh9x"
      },
      "id": "jfRCzToMeh9x"
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ID=\"rscw_oltp_stg_ds\"\n",
        "\n",
        "\n",
        "# 1. Generate the access token\n",
        "token = get_access_token()\n",
        "\n",
        "\n",
        "if token:\n",
        "    print(\"Successfully generated access token.\")\n",
        "    # Run the scan\n",
        "    print(\"\\nStarting the scan for the entire dataset.....\")\n",
        "\n",
        "    dataset_id=DATASET_ID.lower()\n",
        "\n",
        "    scan_type=\"DATA_KNOWLEDGE_ENGINE_SCAN\"\n",
        "\n",
        "\n",
        "    # 2a. Create the scan (does not automatically execute the scan)\n",
        "    scan_id = dataset_id.replace(\"_\",\"-\") + \"-dataset-documentation-scan\"\n",
        "    create_scan_synchronous(token,dataset_id, scan_id,scan_type,\"\",\"\")\n",
        "    print(f\"Successfully created (if it didnt exist) the scan: {scan_id}\")\n",
        "\n",
        "    # 2b. Run the patch below so that programmatically executed scans show up in the Cloud Console UI\n",
        "    patch_source_dataset_with_labels(token, dataset_id, scan_type, scan_id)\n",
        "    print(f\"Successfully patched the dataset with the scan name: {scan_id}\")\n",
        "\n",
        "    # 2c. Execute the scan\n",
        "    run_scan_synchronous(token, scan_id)\n",
        "    print(f\"Successfully ran the scan: {scan_id}\")\n",
        "\n",
        "else:\n",
        "    error_message=\"Failed to get access token. Please check your credentials and permissions.\"\n",
        "    print(error_message)"
      ],
      "metadata": {
        "id": "-WUpPySeec7M"
      },
      "id": "-WUpPySeec7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample command to delete the scan (just FYI - DO NOT RUN THIS):<br>\n",
        "`alias gcurl='curl -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\"'`\n",
        "\n",
        "<br>\n",
        "\n",
        "`gcurl -X DELETE https://dataplex.googleapis.com/v1/projects/data-insights-quickstart/locations/us-central1/dataScans/rscw-oltp-stg-ds-dataset-documentation-scan`\n"
      ],
      "metadata": {
        "id": "NDhGTdiqtwA6"
      },
      "id": "NDhGTdiqtwA6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This concludes the lab module unit on Data Documentation Scan at the dataset level, proceed to the User Manual for instructions on reviewing the results in the Cloud Console - BigQuery UI and catalog.\n",
        "\n",
        "**Note:** The execution of this notebook should complete to run the Module 3 that is Data Engineering Agent focused"
      ],
      "metadata": {
        "id": "F29xTWeL4WiV"
      },
      "id": "F29xTWeL4WiV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Module_02c_Data_Documentation_Scan",
      "collapsed_sections": [
        "ZtC44VdTdsl_"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}